{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea068758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from my_lib.plotting import plot_probability_distributions, plot_learning_curve, plot_validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14219a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/titanic_newAge_withEncoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97ccfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = data[(data['NewAge']>0)]['NewAge'].mean()\n",
    "\n",
    "def replace_age(age):\n",
    "    return mean_age if age < 0 else age\n",
    "\n",
    "data['NewAge'] = data['NewAge'].map(replace_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c906aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Unnamed: 0', 'PassengerId', 'Survived'])\n",
    "y = data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0df68ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c0fb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c54a3f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler', StandardScaler()),\n",
       "  ('logisticregression', LogisticRegression(random_state=1))],\n",
       " 'verbose': False,\n",
       " 'standardscaler': StandardScaler(),\n",
       " 'logisticregression': LogisticRegression(random_state=1),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'logisticregression__C': 1.0,\n",
       " 'logisticregression__class_weight': None,\n",
       " 'logisticregression__dual': False,\n",
       " 'logisticregression__fit_intercept': True,\n",
       " 'logisticregression__intercept_scaling': 1,\n",
       " 'logisticregression__l1_ratio': None,\n",
       " 'logisticregression__max_iter': 100,\n",
       " 'logisticregression__multi_class': 'auto',\n",
       " 'logisticregression__n_jobs': None,\n",
       " 'logisticregression__penalty': 'l2',\n",
       " 'logisticregression__random_state': 1,\n",
       " 'logisticregression__solver': 'lbfgs',\n",
       " 'logisticregression__tol': 0.0001,\n",
       " 'logisticregression__verbose': 0,\n",
       " 'logisticregression__warm_start': False}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd4e02",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "171bd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'logisticregression__C':[0.01, 0.1, 1, 10, 100],\n",
    "    'logisticregression__penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'logisticregression__class_weight': [None, 'balanced'],\n",
    "    'logisticregression__fit_intercept': [True, False],\n",
    "    'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42a74fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    clf,\n",
    "    param_grid=parameters,\n",
    "    n_jobs=5,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8bc9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "900 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.79181523 0.79181523        nan 0.79181523 0.79181523        nan\n",
      "        nan 0.77493352        nan 0.62447552 0.78336452 0.78336452\n",
      " 0.78760957 0.78336452 0.78336452        nan        nan        nan\n",
      "        nan        nan 0.76511376 0.76511376        nan 0.76511376\n",
      " 0.76511376        nan        nan 0.77493352        nan 0.77493352\n",
      " 0.76087856 0.76087856 0.76087856 0.76087856 0.76087856        nan\n",
      "        nan        nan        nan        nan 0.77354477 0.77354477\n",
      "        nan 0.77354477 0.77354477        nan        nan 0.77493352\n",
      "        nan 0.77493352 0.76790111 0.76790111 0.76368561 0.76790111\n",
      " 0.76790111        nan        nan        nan        nan        nan\n",
      " 0.76793066 0.76793066        nan 0.76793066 0.76793066        nan\n",
      "        nan 0.77493352        nan 0.77493352 0.76228701 0.76228701\n",
      " 0.76228701 0.76228701 0.76228701        nan        nan        nan\n",
      "        nan        nan 0.79181523 0.79181523        nan 0.79181523\n",
      " 0.79181523        nan        nan 0.79322368        nan 0.79182508\n",
      " 0.78898848 0.78898848 0.78477297 0.78898848 0.78898848        nan\n",
      "        nan        nan        nan        nan 0.76511376 0.76511376\n",
      "        nan 0.76511376 0.76511376        nan        nan 0.76649266\n",
      "        nan 0.76649266 0.76369546 0.76369546 0.76369546 0.76369546\n",
      " 0.76369546        nan        nan        nan        nan        nan\n",
      " 0.77354477 0.77354477        nan 0.77354477 0.77354477        nan\n",
      "        nan 0.76650251        nan 0.76930956 0.77354477 0.77354477\n",
      " 0.77214616 0.77354477 0.77354477        nan        nan        nan\n",
      "        nan        nan 0.76793066 0.76793066        nan 0.76793066\n",
      " 0.76793066        nan        nan 0.76649266        nan 0.76649266\n",
      " 0.76370531 0.76370531 0.76370531 0.76370531 0.76370531        nan\n",
      "        nan        nan        nan        nan 0.79181523 0.79181523\n",
      "        nan 0.79181523 0.79181523        nan        nan 0.78900817\n",
      "        nan 0.79181523 0.79040678 0.79040678 0.79040678 0.79040678\n",
      " 0.79040678        nan        nan        nan        nan        nan\n",
      " 0.76511376 0.76511376        nan 0.76511376 0.76511376        nan\n",
      "        nan 0.76512361        nan 0.76512361 0.76653206 0.76653206\n",
      " 0.76653206 0.76653206 0.76653206        nan        nan        nan\n",
      "        nan        nan 0.77354477 0.77354477        nan 0.77354477\n",
      " 0.77354477        nan        nan 0.77354477        nan 0.77213631\n",
      " 0.77354477 0.77354477 0.77354477 0.77354477 0.77354477        nan\n",
      "        nan        nan        nan        nan 0.76793066 0.76793066\n",
      "        nan 0.76793066 0.76793066        nan        nan 0.76794051\n",
      "        nan 0.76794051 0.76794051 0.76794051 0.76794051 0.76794051\n",
      " 0.76794051        nan        nan        nan        nan        nan\n",
      " 0.79181523 0.79181523        nan 0.79181523 0.79181523        nan\n",
      "        nan 0.79181523        nan 0.79181523 0.79040678 0.79040678\n",
      " 0.79181523 0.79040678 0.79040678        nan        nan        nan\n",
      "        nan        nan 0.76511376 0.76511376        nan 0.76511376\n",
      " 0.76511376        nan        nan 0.76652221        nan 0.76652221\n",
      " 0.76511376 0.76511376 0.76511376 0.76511376 0.76511376        nan\n",
      "        nan        nan        nan        nan 0.77354477 0.77354477\n",
      "        nan 0.77354477 0.77354477        nan        nan 0.77354477\n",
      "        nan 0.77354477 0.77354477 0.77354477 0.77354477 0.77354477\n",
      " 0.77354477        nan        nan        nan        nan        nan\n",
      " 0.76793066 0.76793066        nan 0.76793066 0.76793066        nan\n",
      "        nan 0.76793066        nan 0.76793066 0.76793066 0.76793066\n",
      " 0.76793066 0.76793066 0.76793066        nan        nan        nan\n",
      "        nan        nan 0.79181523 0.79181523        nan 0.79181523\n",
      " 0.79181523        nan        nan 0.79181523        nan 0.79181523\n",
      " 0.79181523 0.79181523 0.79181523 0.79181523 0.79181523        nan\n",
      "        nan        nan        nan        nan 0.76511376 0.76511376\n",
      "        nan 0.76511376 0.76511376        nan        nan 0.76511376\n",
      "        nan 0.76511376 0.76511376 0.76511376 0.76511376 0.76511376\n",
      " 0.76511376        nan        nan        nan        nan        nan\n",
      " 0.77354477 0.77354477        nan 0.77354477 0.77354477        nan\n",
      "        nan 0.77354477        nan 0.77354477 0.77354477 0.77354477\n",
      " 0.77354477 0.77354477 0.77354477        nan        nan        nan\n",
      "        nan        nan 0.76793066 0.76793066        nan 0.76793066\n",
      " 0.76793066        nan        nan 0.76793066        nan 0.76793066\n",
      " 0.76793066 0.76793066 0.76793066 0.76793066 0.76793066        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "/home/nb315/anaconda3/envs/data-science/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.80555274 0.80555274        nan 0.80555274 0.80555274        nan\n",
      "        nan 0.77496287        nan 0.62447276 0.78656341 0.78656341\n",
      " 0.79113901 0.78656341 0.78656341        nan        nan        nan\n",
      "        nan        nan 0.77391148 0.77391148        nan 0.77391148\n",
      " 0.77391148        nan        nan 0.77496287        nan 0.77496287\n",
      " 0.76336357 0.76336357 0.76336357 0.76336357 0.76336357        nan\n",
      "        nan        nan        nan        nan 0.78235043 0.78235043\n",
      "        nan 0.78235043 0.78235043        nan        nan 0.77496287\n",
      "        nan 0.77496287 0.77320788 0.77320788 0.76969294 0.77320788\n",
      " 0.77320788        nan        nan        nan        nan        nan\n",
      " 0.77742766 0.77742766        nan 0.77742766 0.77742766        nan\n",
      "        nan 0.77496287        nan 0.77496287 0.76406594 0.76406594\n",
      " 0.76406594 0.76441805 0.76441805        nan        nan        nan\n",
      "        nan        nan 0.80555274 0.80555274        nan 0.80555274\n",
      " 0.80555274        nan        nan 0.79114149        nan 0.79324798\n",
      " 0.80484913 0.80484913 0.80098208 0.80484913 0.80484913        nan\n",
      "        nan        nan        nan        nan 0.77391148 0.77391148\n",
      "        nan 0.77391148 0.77391148        nan        nan 0.76933835\n",
      "        nan 0.76933835 0.77250551 0.77250551 0.77250551 0.77250551\n",
      " 0.77250551        nan        nan        nan        nan        nan\n",
      " 0.78235043 0.78235043        nan 0.78235043 0.78235043        nan\n",
      "        nan 0.77215092        nan 0.77637132 0.78129533 0.78129533\n",
      " 0.78164868 0.78129533 0.78129533        nan        nan        nan\n",
      "        nan        nan 0.77742766 0.77742766        nan 0.77742766\n",
      " 0.77742766        nan        nan 0.77074556        nan 0.77074556\n",
      " 0.77391025 0.77391025 0.77391025 0.77391025 0.77391025        nan\n",
      "        nan        nan        nan        nan 0.80555274 0.80555274\n",
      "        nan 0.80555274 0.80555274        nan        nan 0.80414676\n",
      "        nan 0.80449825 0.80520186 0.80520186 0.80590485 0.80520186\n",
      " 0.80520186        nan        nan        nan        nan        nan\n",
      " 0.77391148 0.77391148        nan 0.77391148 0.77391148        nan\n",
      "        nan 0.77426236        nan 0.77426236 0.77426298 0.77426298\n",
      " 0.77426298 0.77426298 0.77426298        nan        nan        nan\n",
      "        nan        nan 0.78235043 0.78235043        nan 0.78235043\n",
      " 0.78235043        nan        nan 0.7816462         nan 0.7816462\n",
      " 0.7809426  0.7809426  0.7805911  0.7809426  0.7809426         nan\n",
      "        nan        nan        nan        nan 0.77742766 0.77742766\n",
      "        nan 0.77742766 0.77742766        nan        nan 0.77637256\n",
      "        nan 0.77637256 0.77637256 0.77637256 0.77637256 0.77637256\n",
      " 0.77637256        nan        nan        nan        nan        nan\n",
      " 0.80555274 0.80555274        nan 0.80555274 0.80555274        nan\n",
      "        nan 0.80555274        nan 0.80555274 0.80555274 0.80555274\n",
      " 0.80555274 0.80555274 0.80555274        nan        nan        nan\n",
      "        nan        nan 0.77391148 0.77391148        nan 0.77391148\n",
      " 0.77391148        nan        nan 0.77391148        nan 0.77391148\n",
      " 0.77391148 0.77391148 0.77391148 0.77391148 0.77391148        nan\n",
      "        nan        nan        nan        nan 0.78235043 0.78235043\n",
      "        nan 0.78235043 0.78235043        nan        nan 0.78199894\n",
      "        nan 0.78199894 0.78199894 0.78199894 0.78199894 0.78199894\n",
      " 0.78199894        nan        nan        nan        nan        nan\n",
      " 0.77742766 0.77742766        nan 0.77742766 0.77742766        nan\n",
      "        nan 0.77672467        nan 0.77672467 0.77707617 0.77707617\n",
      " 0.77707617 0.77707617 0.77707617        nan        nan        nan\n",
      "        nan        nan 0.80555274 0.80555274        nan 0.80555274\n",
      " 0.80555274        nan        nan 0.80555274        nan 0.80555274\n",
      " 0.80555274 0.80555274 0.80555274 0.80555274 0.80555274        nan\n",
      "        nan        nan        nan        nan 0.77391148 0.77391148\n",
      "        nan 0.77391148 0.77391148        nan        nan 0.77391148\n",
      "        nan 0.77391148 0.77391148 0.77391148 0.77391148 0.77391148\n",
      " 0.77391148        nan        nan        nan        nan        nan\n",
      " 0.78235043 0.78235043        nan 0.78235043 0.78235043        nan\n",
      "        nan 0.78235043        nan 0.78235043 0.78235043 0.78235043\n",
      " 0.78235043 0.78235043 0.78235043        nan        nan        nan\n",
      "        nan        nan 0.77742766 0.77742766        nan 0.77742766\n",
      " 0.77742766        nan        nan 0.77707617        nan 0.77707617\n",
      " 0.77742766 0.77742766 0.77742766 0.77742766 0.77742766        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression(random_state=1))]),\n",
       "             n_jobs=5,\n",
       "             param_grid={'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n",
       "                         'logisticregression__class_weight': [None, 'balanced'],\n",
       "                         'logisticregression__fit_intercept': [True, False],\n",
       "                         'logisticregression__penalty': ['none', 'l1', 'l2',\n",
       "                                                         'elasticnet'],\n",
       "                         'logisticregression__solver': ['newton-cg', 'lbfgs',\n",
       "                                                        'liblinear', 'sag',\n",
       "                                                        'saga']},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "797b689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01724925, 0.01113224, 0.00523291, 0.01008458, 0.00916471,\n",
       "        0.00390458, 0.00463924, 0.00600781, 0.00587811, 0.00750155,\n",
       "        0.0101059 , 0.01014857, 0.00734525, 0.00894928, 0.00915785,\n",
       "        0.00628185, 0.00617614, 0.00470448, 0.00429993, 0.00401168,\n",
       "        0.01020427, 0.00747795, 0.00534687, 0.00963082, 0.00945706,\n",
       "        0.00552411, 0.00536723, 0.00460596, 0.00469122, 0.00718031,\n",
       "        0.0082325 , 0.00632443, 0.00458817, 0.00917902, 0.00941868,\n",
       "        0.00469251, 0.00474668, 0.00363345, 0.00391622, 0.00464587,\n",
       "        0.01733451, 0.01285839, 0.00531926, 0.00766296, 0.00971446,\n",
       "        0.00597682, 0.00560699, 0.00744095, 0.00607252, 0.01005521,\n",
       "        0.00934668, 0.01019573, 0.0060338 , 0.00716505, 0.00656223,\n",
       "        0.00455065, 0.00490785, 0.00372643, 0.00464506, 0.00602245,\n",
       "        0.0158217 , 0.01151218, 0.00578685, 0.01175475, 0.01011181,\n",
       "        0.00562663, 0.00556321, 0.00719323, 0.00575995, 0.00898876,\n",
       "        0.00849261, 0.00661263, 0.00467048, 0.006705  , 0.00660715,\n",
       "        0.00495834, 0.00476575, 0.00375738, 0.003653  , 0.00360079,\n",
       "        0.01039863, 0.00728278, 0.00365429, 0.0072648 , 0.00820918,\n",
       "        0.00637841, 0.00390415, 0.00632038, 0.00565295, 0.01021638,\n",
       "        0.01632366, 0.01113496, 0.00781326, 0.01007996, 0.00612631,\n",
       "        0.00488343, 0.00368629, 0.00377555, 0.00552192, 0.00589809,\n",
       "        0.01474385, 0.01176991, 0.00591269, 0.01195636, 0.00946813,\n",
       "        0.00599852, 0.00593462, 0.00595503, 0.00366182, 0.00735483,\n",
       "        0.01337161, 0.00704923, 0.00535111, 0.00785241, 0.006847  ,\n",
       "        0.0044682 , 0.00669804, 0.00962811, 0.00603857, 0.0048974 ,\n",
       "        0.01559629, 0.0120307 , 0.00608454, 0.01211157, 0.01002512,\n",
       "        0.00615497, 0.00608025, 0.00834556, 0.00552826, 0.01046014,\n",
       "        0.01706557, 0.01162353, 0.00764718, 0.01065655, 0.00962138,\n",
       "        0.00583487, 0.00637689, 0.00577025, 0.00562568, 0.00426636,\n",
       "        0.0094727 , 0.00736313, 0.00667753, 0.00966482, 0.00867791,\n",
       "        0.00546131, 0.00536656, 0.00757833, 0.00607243, 0.01060896,\n",
       "        0.01467218, 0.01204739, 0.0079246 , 0.01033001, 0.01033926,\n",
       "        0.00557766, 0.00405397, 0.00355854, 0.00566921, 0.00600848,\n",
       "        0.01687508, 0.01075859, 0.00566831, 0.01117969, 0.00814018,\n",
       "        0.00357008, 0.00405779, 0.00630426, 0.00369039, 0.00658722,\n",
       "        0.0114171 , 0.01161442, 0.0050262 , 0.00681901, 0.00617447,\n",
       "        0.00379453, 0.0036345 , 0.00544453, 0.00544209, 0.00562315,\n",
       "        0.01430359, 0.01103482, 0.00576844, 0.00941534, 0.00604687,\n",
       "        0.00361257, 0.00386982, 0.0078198 , 0.00378652, 0.00807514,\n",
       "        0.0127183 , 0.00730605, 0.0046947 , 0.00690684, 0.00645871,\n",
       "        0.00361552, 0.00495191, 0.00578909, 0.00558076, 0.00559335,\n",
       "        0.01624336, 0.01240673, 0.00561852, 0.00753431, 0.00649953,\n",
       "        0.00374832, 0.00389752, 0.00519881, 0.00469093, 0.01103921,\n",
       "        0.0166203 , 0.0122354 , 0.00766816, 0.01143317, 0.01038055,\n",
       "        0.00550938, 0.00365438, 0.00362072, 0.00357914, 0.0038784 ,\n",
       "        0.01076512, 0.0080483 , 0.00369935, 0.00857692, 0.00766478,\n",
       "        0.0037107 , 0.00395422, 0.00519776, 0.00404644, 0.00772519,\n",
       "        0.00972486, 0.00733299, 0.00493145, 0.00763078, 0.00768914,\n",
       "        0.00576525, 0.00560694, 0.00566168, 0.00557394, 0.00570579,\n",
       "        0.0166431 , 0.00836697, 0.00357609, 0.00969596, 0.00796399,\n",
       "        0.00500312, 0.00573215, 0.00658984, 0.0036098 , 0.00726109,\n",
       "        0.01210275, 0.00989051, 0.00753598, 0.01124263, 0.01076779,\n",
       "        0.00474572, 0.00367508, 0.00367632, 0.00391788, 0.00476708,\n",
       "        0.01181364, 0.00753932, 0.00376801, 0.00766211, 0.00635533,\n",
       "        0.00390663, 0.0036561 , 0.00616436, 0.00363393, 0.00711045,\n",
       "        0.00931859, 0.0072618 , 0.0058567 , 0.01136909, 0.00963178,\n",
       "        0.00550618, 0.00555115, 0.00566378, 0.00562377, 0.00493164,\n",
       "        0.01370859, 0.00972624, 0.00421009, 0.00751977, 0.00855646,\n",
       "        0.00452752, 0.00580044, 0.00816488, 0.00568085, 0.01130238,\n",
       "        0.01728516, 0.01208076, 0.00616426, 0.00795522, 0.00689163,\n",
       "        0.00367837, 0.0035923 , 0.00459366, 0.00506411, 0.00558929,\n",
       "        0.01529202, 0.01251578, 0.00567322, 0.01170001, 0.01014004,\n",
       "        0.0058497 , 0.00434985, 0.00638919, 0.00575829, 0.00844469,\n",
       "        0.01174006, 0.00802484, 0.00705619, 0.00779037, 0.0065012 ,\n",
       "        0.00386033, 0.00356841, 0.00371895, 0.00373964, 0.00369191,\n",
       "        0.01110349, 0.0070271 , 0.00395446, 0.00745702, 0.00873055,\n",
       "        0.0057044 , 0.00571852, 0.00809717, 0.00593357, 0.01116819,\n",
       "        0.0160428 , 0.00729342, 0.00503302, 0.0092216 , 0.00804844,\n",
       "        0.0047369 , 0.00429139, 0.00376401, 0.00377274, 0.00502868,\n",
       "        0.01109505, 0.00886531, 0.00427208, 0.00965519, 0.00974884,\n",
       "        0.00572772, 0.00586104, 0.00819798, 0.00617137, 0.01017952,\n",
       "        0.01127315, 0.0104466 , 0.0054234 , 0.0073781 , 0.0062243 ,\n",
       "        0.00363941, 0.00390782, 0.00395885, 0.0040802 , 0.00417337,\n",
       "        0.01021276, 0.00794048, 0.00406532, 0.00749912, 0.00665693,\n",
       "        0.00385237, 0.00392137, 0.0061327 , 0.00376215, 0.01057816,\n",
       "        0.01672339, 0.01256256, 0.00784969, 0.01103501, 0.00876698,\n",
       "        0.0043509 , 0.00569124, 0.00573983, 0.00597048, 0.00603523,\n",
       "        0.01557026, 0.01209249, 0.00376005, 0.00790753, 0.00835667,\n",
       "        0.0041636 , 0.00452332, 0.00724545, 0.00517879, 0.01031413,\n",
       "        0.01342244, 0.01030903, 0.00768657, 0.01108847, 0.01051006,\n",
       "        0.00508294, 0.00594101, 0.0056221 , 0.00548997, 0.00606403]),\n",
       " 'std_fit_time': array([1.18406822e-03, 8.31146836e-04, 7.73373110e-04, 1.23448470e-03,\n",
       "        1.21625122e-03, 4.10980992e-04, 6.04167086e-04, 1.23077834e-03,\n",
       "        2.94419766e-04, 1.55333313e-03, 1.85794946e-03, 3.50418196e-04,\n",
       "        2.91195532e-04, 1.59073396e-03, 1.37118034e-03, 4.09175935e-04,\n",
       "        3.25589517e-04, 6.70054401e-04, 7.48862781e-04, 3.59867566e-04,\n",
       "        8.27986245e-04, 2.70332203e-04, 2.84716692e-04, 1.82349432e-03,\n",
       "        7.92126064e-04, 1.28105779e-04, 8.88956778e-04, 2.22526287e-04,\n",
       "        9.24861072e-04, 1.56974628e-03, 3.06156735e-04, 2.40803313e-04,\n",
       "        5.82107444e-05, 8.84922340e-04, 9.03573577e-05, 8.56327854e-04,\n",
       "        1.11406606e-03, 5.89634549e-05, 4.05424843e-04, 8.72357303e-04,\n",
       "        1.32475137e-03, 5.62786458e-04, 7.68907755e-04, 3.18584087e-04,\n",
       "        1.99851195e-03, 3.64502184e-04, 2.90018914e-04, 4.30049189e-04,\n",
       "        3.59366672e-04, 2.10158804e-03, 7.60401818e-04, 1.78153088e-03,\n",
       "        7.45938964e-04, 4.46938647e-04, 2.94444571e-04, 9.37344206e-04,\n",
       "        7.98963049e-04, 2.24555138e-04, 9.39849636e-04, 3.05992278e-04,\n",
       "        1.05630757e-03, 4.39707018e-04, 5.72193464e-05, 4.13254933e-04,\n",
       "        1.13354834e-04, 6.48038013e-05, 1.20643702e-04, 1.56461426e-04,\n",
       "        9.00035662e-05, 9.43682934e-04, 3.38627398e-04, 3.60055618e-04,\n",
       "        4.88053023e-05, 1.11311034e-03, 7.57598070e-04, 1.52421255e-03,\n",
       "        1.06280087e-03, 1.88318781e-04, 7.41133501e-05, 5.51363571e-05,\n",
       "        6.76559457e-04, 3.50638417e-04, 6.69765954e-05, 1.61028932e-04,\n",
       "        1.50203580e-03, 4.61414132e-04, 4.74016343e-04, 1.29543562e-03,\n",
       "        3.41134915e-05, 2.10252346e-04, 7.27899670e-04, 4.55279729e-04,\n",
       "        4.54134397e-04, 3.57715456e-04, 7.92010013e-05, 7.98254366e-04,\n",
       "        1.04727227e-04, 1.68369735e-04, 3.65108349e-04, 2.24736272e-04,\n",
       "        1.08249904e-03, 6.58030831e-04, 1.20349637e-04, 7.89890340e-04,\n",
       "        4.04900999e-04, 1.01352475e-04, 5.64104900e-04, 1.39960944e-03,\n",
       "        1.00243760e-04, 9.55756401e-04, 1.93424700e-03, 7.61536002e-04,\n",
       "        4.52001470e-04, 1.44539064e-03, 9.30889380e-04, 1.28181102e-03,\n",
       "        1.61598389e-04, 2.82906585e-03, 5.24358867e-04, 8.90271185e-04,\n",
       "        5.16106140e-03, 4.54119182e-04, 2.45474667e-04, 5.09247297e-04,\n",
       "        2.33547743e-04, 2.79032434e-04, 2.13221221e-04, 3.48703271e-04,\n",
       "        1.02981968e-04, 2.78410104e-04, 1.18759111e-03, 9.24703668e-04,\n",
       "        3.99754622e-04, 4.52955932e-04, 1.60983784e-04, 9.57179067e-05,\n",
       "        5.35078831e-04, 2.40369984e-04, 2.81193965e-04, 5.22328337e-04,\n",
       "        2.73830671e-04, 2.47753015e-04, 1.76117634e-04, 2.24084669e-03,\n",
       "        1.66054357e-03, 1.48198549e-04, 9.62541602e-05, 1.86123134e-04,\n",
       "        2.89164078e-04, 7.42588358e-04, 9.37629757e-04, 4.60916520e-04,\n",
       "        5.22257025e-04, 1.68495347e-04, 3.19858548e-04, 1.30635507e-04,\n",
       "        7.20755502e-04, 3.73904189e-05, 7.83318209e-05, 5.80541761e-04,\n",
       "        9.21553600e-04, 1.31198640e-04, 7.19059684e-05, 2.58847835e-04,\n",
       "        1.94746250e-03, 1.05234407e-04, 1.73701940e-04, 9.68013103e-04,\n",
       "        1.61681453e-04, 1.74382576e-04, 3.45205403e-03, 6.22572034e-04,\n",
       "        3.39968896e-04, 1.70006056e-04, 1.28208078e-04, 1.44977129e-04,\n",
       "        8.43076414e-05, 8.84006542e-04, 6.39094154e-05, 1.09198978e-04,\n",
       "        6.64364151e-04, 1.29594357e-04, 2.45293979e-04, 1.96197364e-03,\n",
       "        6.81863131e-05, 6.84699639e-05, 5.76945876e-04, 4.61406420e-05,\n",
       "        4.49997732e-04, 1.74978127e-03, 2.06003959e-03, 2.90745505e-04,\n",
       "        3.50444494e-05, 2.39902437e-04, 3.85141092e-04, 5.57211509e-05,\n",
       "        1.08939654e-03, 2.11729466e-04, 1.43583388e-04, 1.16956345e-04,\n",
       "        6.49105037e-04, 2.55063829e-04, 7.87793959e-04, 2.74587554e-04,\n",
       "        1.64358236e-04, 6.33437560e-05, 2.81990570e-04, 7.83573605e-05,\n",
       "        9.57467112e-04, 1.57100614e-04, 5.92900703e-04, 6.59557720e-04,\n",
       "        9.76541778e-05, 2.80523401e-04, 2.96996538e-04, 5.74568534e-04,\n",
       "        6.40961379e-05, 1.66348222e-04, 5.45233631e-05, 4.20964941e-04,\n",
       "        2.53518066e-03, 6.28502795e-04, 4.86316397e-05, 2.04932224e-03,\n",
       "        1.94099699e-03, 1.85829956e-04, 2.98836910e-04, 1.50906870e-04,\n",
       "        6.02346523e-04, 6.86638825e-04, 2.02107417e-04, 2.14144770e-04,\n",
       "        1.66227171e-04, 2.83076756e-04, 1.38292171e-03, 1.91150534e-04,\n",
       "        6.63421535e-05, 2.47920676e-04, 1.29741196e-04, 1.44446356e-04,\n",
       "        9.06506798e-04, 1.94640401e-03, 5.34683950e-05, 2.06334961e-03,\n",
       "        1.39481379e-03, 9.74278071e-04, 7.77159142e-05, 1.19547379e-03,\n",
       "        1.09677273e-04, 3.82950157e-04, 1.71163363e-03, 1.79394181e-03,\n",
       "        5.38995598e-05, 1.34801704e-04, 1.04196325e-03, 7.26177686e-04,\n",
       "        7.89901442e-05, 5.77710602e-05, 6.57182715e-04, 1.00080291e-03,\n",
       "        2.35232332e-03, 7.60744789e-04, 3.03930798e-04, 6.78885747e-04,\n",
       "        3.76357133e-04, 2.37588994e-04, 7.19712044e-05, 4.74053541e-04,\n",
       "        7.56182251e-05, 8.63949357e-04, 4.93110006e-04, 2.43855259e-04,\n",
       "        1.03461051e-03, 2.75743001e-04, 4.28880859e-04, 1.54286158e-04,\n",
       "        1.05052475e-04, 1.87116794e-04, 6.80743121e-05, 1.04366062e-03,\n",
       "        2.97390142e-03, 2.24915256e-03, 4.76867427e-04, 1.92529012e-04,\n",
       "        2.38496685e-03, 9.64620362e-04, 1.98180854e-04, 1.10870433e-04,\n",
       "        1.05051934e-04, 5.68312093e-04, 9.26383866e-04, 2.97859628e-04,\n",
       "        1.12359330e-03, 4.63097821e-04, 6.40264784e-04, 6.82995619e-05,\n",
       "        5.54363752e-05, 5.20465331e-04, 9.00089061e-04, 7.57532631e-05,\n",
       "        7.02440204e-04, 7.27805835e-04, 4.17346666e-05, 4.34038246e-04,\n",
       "        2.61468585e-04, 3.08899682e-04, 7.96583220e-04, 1.41453393e-03,\n",
       "        9.14114368e-05, 1.17008001e-03, 2.40936296e-03, 7.56528136e-04,\n",
       "        1.13118349e-03, 9.43203683e-04, 3.12432679e-04, 3.83352034e-04,\n",
       "        7.07352205e-05, 7.31214818e-05, 7.67167713e-05, 7.86704724e-05,\n",
       "        2.04122292e-03, 1.96784831e-04, 3.58648478e-04, 4.76297335e-04,\n",
       "        1.34624122e-03, 2.45541923e-04, 1.43639657e-04, 3.32111758e-04,\n",
       "        5.82229913e-04, 2.29234394e-04, 2.91152637e-03, 6.49694896e-04,\n",
       "        7.22951810e-05, 1.72033319e-03, 1.51504754e-03, 9.65469105e-04,\n",
       "        6.37270315e-04, 1.51698426e-04, 2.39934963e-04, 1.06082459e-03,\n",
       "        1.49497623e-03, 1.99346042e-03, 7.41162133e-04, 1.79883445e-03,\n",
       "        2.36574413e-04, 1.87514622e-04, 2.46515172e-04, 3.47746012e-04,\n",
       "        4.08154687e-04, 2.26765675e-04, 1.70367074e-03, 1.49059234e-03,\n",
       "        5.79520850e-04, 4.69676637e-04, 4.35568274e-04, 8.86144353e-05,\n",
       "        3.21407685e-04, 2.85417914e-04, 3.58245875e-04, 5.06648997e-04,\n",
       "        2.65988580e-04, 3.75801999e-04, 2.48659372e-04, 3.25223273e-04,\n",
       "        1.92725782e-04, 1.45979162e-04, 2.42767936e-04, 9.18359914e-04,\n",
       "        7.04502656e-05, 9.70884917e-04, 1.43961629e-03, 6.08336867e-04,\n",
       "        1.61642662e-04, 1.20235373e-03, 1.35981462e-03, 1.15794429e-03,\n",
       "        6.95011055e-05, 4.74654447e-05, 4.46316702e-04, 5.46468544e-04,\n",
       "        7.23053619e-04, 1.87545007e-04, 5.65968547e-05, 5.02456634e-04,\n",
       "        1.63118851e-03, 2.58457291e-04, 6.94731258e-04, 8.10418939e-04,\n",
       "        5.25661736e-04, 1.55242210e-03, 2.80171706e-03, 2.24621690e-03,\n",
       "        1.22991962e-03, 1.64778599e-03, 1.53937292e-03, 1.15246151e-03,\n",
       "        5.49613017e-04, 8.01801992e-04, 9.30970444e-04, 5.47816141e-04]),\n",
       " 'mean_score_time': array([0.00330267, 0.00251737, 0.        , 0.00206513, 0.00219793,\n",
       "        0.        , 0.        , 0.00205178, 0.        , 0.00208759,\n",
       "        0.0017632 , 0.0024272 , 0.00263968, 0.00207686, 0.00258522,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0016984 , 0.00170074, 0.        , 0.00195889, 0.00223265,\n",
       "        0.        , 0.        , 0.0015584 , 0.        , 0.00178928,\n",
       "        0.00160294, 0.00168862, 0.00179906, 0.00254002, 0.00256472,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00251822, 0.00243526, 0.        , 0.00161061, 0.00242119,\n",
       "        0.        , 0.        , 0.00240612, 0.        , 0.00235639,\n",
       "        0.00164266, 0.00241418, 0.00187693, 0.00170131, 0.00186892,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00249348, 0.00229206, 0.        , 0.00260906, 0.00248833,\n",
       "        0.        , 0.        , 0.00247598, 0.        , 0.00219226,\n",
       "        0.00158505, 0.00168743, 0.00157747, 0.00193362, 0.00168595,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00164633, 0.00158572, 0.        , 0.00171824, 0.00203471,\n",
       "        0.        , 0.        , 0.002177  , 0.        , 0.00249281,\n",
       "        0.00253348, 0.00254636, 0.00263462, 0.00249219, 0.0017014 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00232472, 0.00258746, 0.        , 0.00273423, 0.00242343,\n",
       "        0.        , 0.        , 0.0017036 , 0.        , 0.00216594,\n",
       "        0.00240965, 0.00156779, 0.00179472, 0.00241919, 0.00179634,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00239577, 0.00252948, 0.        , 0.00248466, 0.00247583,\n",
       "        0.        , 0.        , 0.00277262, 0.        , 0.00239825,\n",
       "        0.00264616, 0.00247202, 0.00249958, 0.0025208 , 0.00237355,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00157509, 0.00172896, 0.        , 0.00209188, 0.00224481,\n",
       "        0.        , 0.        , 0.00246806, 0.        , 0.00258684,\n",
       "        0.00240536, 0.00261488, 0.00245543, 0.00249443, 0.00257993,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00256925, 0.00243692, 0.        , 0.00249438, 0.0018939 ,\n",
       "        0.        , 0.        , 0.00200605, 0.        , 0.0015533 ,\n",
       "        0.00179086, 0.00244102, 0.00156884, 0.00158858, 0.00158033,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00241079, 0.00243564, 0.        , 0.0020987 , 0.00175066,\n",
       "        0.        , 0.        , 0.00241184, 0.        , 0.00204215,\n",
       "        0.00209789, 0.00175729, 0.00159359, 0.00171361, 0.00163531,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0023881 , 0.0025538 , 0.        , 0.00165696, 0.00157614,\n",
       "        0.        , 0.        , 0.00158534, 0.        , 0.00248446,\n",
       "        0.00246215, 0.00239806, 0.00246968, 0.00247617, 0.00253201,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00159845, 0.00182323, 0.        , 0.00226803, 0.001755  ,\n",
       "        0.        , 0.        , 0.0016294 , 0.        , 0.00167599,\n",
       "        0.00165906, 0.0016737 , 0.00170865, 0.00174351, 0.0019516 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00251827, 0.0020165 , 0.        , 0.00209336, 0.00185905,\n",
       "        0.        , 0.        , 0.00231671, 0.        , 0.00173764,\n",
       "        0.00181899, 0.00237002, 0.00240312, 0.00255203, 0.00272713,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00192814, 0.00156384, 0.        , 0.00160341, 0.00162506,\n",
       "        0.        , 0.        , 0.00210733, 0.        , 0.0017746 ,\n",
       "        0.00161223, 0.00159807, 0.00209236, 0.0025105 , 0.00245872,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00226278, 0.0017879 , 0.        , 0.00164051, 0.00231385,\n",
       "        0.        , 0.        , 0.00243831, 0.        , 0.00247059,\n",
       "        0.00248485, 0.00253158, 0.00214171, 0.00189548, 0.00161338,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00238485, 0.00261669, 0.        , 0.00244122, 0.00230613,\n",
       "        0.        , 0.        , 0.00192375, 0.        , 0.0020196 ,\n",
       "        0.00197659, 0.00177732, 0.00207081, 0.00178747, 0.00160403,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00189505, 0.00168309, 0.        , 0.00165839, 0.00228205,\n",
       "        0.        , 0.        , 0.00242147, 0.        , 0.002457  ,\n",
       "        0.00234575, 0.0015543 , 0.00165043, 0.0020637 , 0.00205045,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0017591 , 0.00178142, 0.        , 0.00219221, 0.00261455,\n",
       "        0.        , 0.        , 0.00246615, 0.        , 0.00239038,\n",
       "        0.00193577, 0.00212989, 0.00182691, 0.00164733, 0.00160398,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00156879, 0.00177507, 0.        , 0.00156922, 0.00160728,\n",
       "        0.        , 0.        , 0.00189695, 0.        , 0.00216718,\n",
       "        0.00242124, 0.00252995, 0.00248017, 0.00221105, 0.00206919,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0024796 , 0.0023356 , 0.        , 0.00181098, 0.00189795,\n",
       "        0.        , 0.        , 0.00209575, 0.        , 0.00251665,\n",
       "        0.00214319, 0.00216117, 0.00238113, 0.00227213, 0.0024467 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       " 'std_score_time': array([8.35278572e-04, 4.56885265e-04, 0.00000000e+00, 4.29652332e-04,\n",
       "        5.28213409e-04, 0.00000000e+00, 0.00000000e+00, 3.90354881e-04,\n",
       "        0.00000000e+00, 4.24280797e-04, 3.32237692e-04, 7.77343439e-05,\n",
       "        1.96050981e-04, 3.32533701e-04, 4.61665733e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.52133518e-04, 1.37383199e-04, 0.00000000e+00, 3.68411387e-04,\n",
       "        3.32109279e-04, 0.00000000e+00, 0.00000000e+00, 3.20222336e-05,\n",
       "        0.00000000e+00, 2.69960354e-04, 2.66224638e-05, 1.74892897e-04,\n",
       "        3.25314909e-04, 2.09440190e-04, 1.23249114e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.06410611e-04, 8.50715978e-05, 0.00000000e+00, 2.04248898e-05,\n",
       "        4.26923213e-04, 0.00000000e+00, 0.00000000e+00, 2.30518144e-04,\n",
       "        0.00000000e+00, 5.10223393e-04, 1.09551757e-04, 2.05109850e-04,\n",
       "        1.45177973e-04, 8.44060228e-05, 1.91285614e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.00471351e-05, 7.46070517e-05, 0.00000000e+00, 3.32343506e-04,\n",
       "        7.57241429e-05, 0.00000000e+00, 0.00000000e+00, 1.87611942e-04,\n",
       "        0.00000000e+00, 4.07497207e-04, 4.04253870e-05, 2.87556671e-04,\n",
       "        6.88719597e-05, 4.49846477e-04, 1.54421680e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.65648960e-05, 2.77341379e-05, 0.00000000e+00, 2.32855135e-04,\n",
       "        3.70190076e-04, 0.00000000e+00, 0.00000000e+00, 4.38750771e-04,\n",
       "        0.00000000e+00, 1.13160058e-04, 5.40135489e-05, 1.00408863e-04,\n",
       "        2.35990491e-04, 1.64166869e-04, 1.51646092e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.60501304e-04, 3.48470677e-04, 0.00000000e+00, 3.73970995e-04,\n",
       "        6.05035895e-05, 0.00000000e+00, 0.00000000e+00, 1.02394560e-04,\n",
       "        0.00000000e+00, 5.22213860e-04, 5.22586646e-04, 4.56503783e-05,\n",
       "        2.40738671e-04, 1.03373419e-03, 1.67748479e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.97117651e-04, 1.96583982e-04, 0.00000000e+00, 1.61348801e-04,\n",
       "        8.14393476e-05, 0.00000000e+00, 0.00000000e+00, 3.07964141e-04,\n",
       "        0.00000000e+00, 1.05163081e-04, 3.34889543e-04, 1.25257380e-04,\n",
       "        9.30609745e-05, 1.99988085e-04, 4.59265233e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.14508983e-05, 2.17268192e-04, 0.00000000e+00, 5.31959350e-04,\n",
       "        3.22738050e-04, 0.00000000e+00, 0.00000000e+00, 4.10911914e-05,\n",
       "        0.00000000e+00, 3.18331894e-04, 1.33913958e-04, 2.72899059e-04,\n",
       "        5.93899484e-05, 1.99821318e-04, 1.97652252e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.10504622e-04, 4.74428771e-05, 0.00000000e+00, 2.24201713e-05,\n",
       "        4.40263198e-04, 0.00000000e+00, 0.00000000e+00, 2.55430410e-04,\n",
       "        0.00000000e+00, 2.53939851e-05, 3.04673594e-04, 9.14270562e-05,\n",
       "        3.29088121e-05, 9.48054924e-06, 1.85768315e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.18190224e-05, 5.29384604e-05, 0.00000000e+00, 3.82945977e-04,\n",
       "        3.41808150e-04, 0.00000000e+00, 0.00000000e+00, 8.25998406e-05,\n",
       "        0.00000000e+00, 3.86336591e-04, 4.21932328e-04, 3.70381973e-04,\n",
       "        1.12163412e-04, 1.52041874e-04, 6.30879167e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.65919628e-05, 1.17039639e-04, 0.00000000e+00, 7.73096130e-05,\n",
       "        2.49984580e-05, 0.00000000e+00, 0.00000000e+00, 1.87346643e-05,\n",
       "        0.00000000e+00, 3.32569904e-05, 3.37882113e-05, 7.56621126e-05,\n",
       "        8.10335052e-05, 8.65975402e-05, 5.06949891e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.69646840e-05, 2.87789986e-04, 0.00000000e+00, 8.26055804e-04,\n",
       "        1.54838651e-04, 0.00000000e+00, 0.00000000e+00, 8.16630654e-05,\n",
       "        0.00000000e+00, 7.40208253e-05, 1.31578079e-04, 1.01323352e-04,\n",
       "        2.27812844e-04, 1.35988273e-04, 3.96433323e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.11136677e-04, 5.49824992e-04, 0.00000000e+00, 4.21693669e-04,\n",
       "        2.82480667e-04, 0.00000000e+00, 0.00000000e+00, 5.77572086e-04,\n",
       "        0.00000000e+00, 1.58892659e-04, 2.02524401e-04, 3.93548344e-04,\n",
       "        8.00043517e-05, 2.94562271e-04, 2.36738321e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.52661374e-04, 3.52393306e-05, 0.00000000e+00, 5.82435460e-05,\n",
       "        3.38127646e-05, 0.00000000e+00, 0.00000000e+00, 5.12956526e-04,\n",
       "        0.00000000e+00, 3.95933097e-04, 6.51725635e-05, 2.97927460e-05,\n",
       "        3.50470283e-04, 1.36208716e-04, 2.64249541e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.01684149e-04, 3.71541129e-04, 0.00000000e+00, 6.95540838e-05,\n",
       "        7.02270576e-04, 0.00000000e+00, 0.00000000e+00, 2.07207556e-05,\n",
       "        0.00000000e+00, 4.28115331e-05, 9.90187005e-05, 2.06067505e-04,\n",
       "        4.08537843e-04, 4.57135465e-04, 7.74066959e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.44482374e-05, 2.45540043e-04, 0.00000000e+00, 9.03865430e-05,\n",
       "        7.42894235e-05, 0.00000000e+00, 0.00000000e+00, 3.98112666e-04,\n",
       "        0.00000000e+00, 5.20066582e-04, 3.38331088e-04, 3.65761535e-04,\n",
       "        3.30524264e-04, 2.54192153e-04, 5.22423800e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.19209381e-04, 1.21038385e-04, 0.00000000e+00, 8.05079070e-05,\n",
       "        3.51922544e-04, 0.00000000e+00, 0.00000000e+00, 1.27012861e-04,\n",
       "        0.00000000e+00, 1.05543206e-04, 3.27463120e-04, 5.84049415e-05,\n",
       "        1.00175600e-04, 4.97035905e-04, 5.52719378e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.98009164e-04, 1.77910711e-04, 0.00000000e+00, 4.58720305e-04,\n",
       "        2.26325341e-04, 0.00000000e+00, 0.00000000e+00, 2.81771626e-05,\n",
       "        0.00000000e+00, 1.63061576e-05, 3.19860837e-04, 3.98946019e-04,\n",
       "        2.83117163e-04, 7.82305965e-05, 3.53707763e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.41493315e-05, 1.91567238e-04, 0.00000000e+00, 3.59793780e-05,\n",
       "        9.34586963e-05, 0.00000000e+00, 0.00000000e+00, 3.05890558e-04,\n",
       "        0.00000000e+00, 2.76711828e-04, 4.74237508e-05, 9.05174088e-05,\n",
       "        1.04169615e-04, 3.78933564e-04, 5.03972380e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.35129195e-05, 4.05323926e-05, 0.00000000e+00, 3.66170715e-04,\n",
       "        4.24928976e-04, 0.00000000e+00, 0.00000000e+00, 3.45256558e-04,\n",
       "        0.00000000e+00, 5.03045452e-04, 4.06225140e-04, 4.72715845e-04,\n",
       "        2.43226375e-04, 2.46479290e-04, 1.69229528e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'param_logisticregression__C': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_logisticregression__class_weight': masked_array(data=[None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None,\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced', None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, 'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
       "                    'balanced', 'balanced', 'balanced', 'balanced'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_logisticregression__fit_intercept': masked_array(data=[True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_logisticregression__penalty': masked_array(data=['none', 'none', 'none', 'none', 'none', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'none', 'none', 'none', 'none', 'none',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'none', 'none', 'none',\n",
       "                    'none', 'none', 'l1', 'l1', 'l1', 'l1', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'none',\n",
       "                    'none', 'none', 'none', 'none', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'none', 'none', 'none', 'none', 'none', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'none', 'none', 'none', 'none', 'none',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'none', 'none', 'none',\n",
       "                    'none', 'none', 'l1', 'l1', 'l1', 'l1', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'none',\n",
       "                    'none', 'none', 'none', 'none', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'none', 'none', 'none', 'none', 'none', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'none', 'none', 'none', 'none', 'none',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'none', 'none', 'none',\n",
       "                    'none', 'none', 'l1', 'l1', 'l1', 'l1', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'none',\n",
       "                    'none', 'none', 'none', 'none', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'none', 'none', 'none', 'none', 'none', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'none', 'none', 'none', 'none', 'none',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'none', 'none', 'none',\n",
       "                    'none', 'none', 'l1', 'l1', 'l1', 'l1', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'none',\n",
       "                    'none', 'none', 'none', 'none', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'none', 'none', 'none', 'none', 'none', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'none', 'none', 'none', 'none', 'none',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'none', 'none', 'none',\n",
       "                    'none', 'none', 'l1', 'l1', 'l1', 'l1', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'none',\n",
       "                    'none', 'none', 'none', 'none', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_logisticregression__solver': masked_array(data=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga',\n",
       "                    'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.01,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 0.1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 1,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 10,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': None,\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': True,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'none',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l1',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'l2',\n",
       "   'logisticregression__solver': 'saga'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'newton-cg'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'lbfgs'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'liblinear'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'sag'},\n",
       "  {'logisticregression__C': 100,\n",
       "   'logisticregression__class_weight': 'balanced',\n",
       "   'logisticregression__fit_intercept': False,\n",
       "   'logisticregression__penalty': 'elasticnet',\n",
       "   'logisticregression__solver': 'saga'}],\n",
       " 'split0_test_score': array([0.81118881, 0.81118881,        nan, 0.81118881, 0.81118881,\n",
       "               nan,        nan, 0.7972028 ,        nan, 0.62237762,\n",
       "        0.81118881, 0.81118881, 0.7972028 , 0.81118881, 0.81118881,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.7972028 ,        nan, 0.7972028 ,\n",
       "        0.77622378, 0.77622378, 0.77622378, 0.77622378, 0.77622378,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78321678, 0.78321678,        nan, 0.78321678, 0.78321678,\n",
       "               nan,        nan, 0.7972028 ,        nan, 0.7972028 ,\n",
       "        0.79020979, 0.79020979, 0.78321678, 0.79020979, 0.79020979,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.7972028 ,        nan, 0.7972028 ,\n",
       "        0.77622378, 0.77622378, 0.77622378, 0.77622378, 0.77622378,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81118881, 0.81118881,        nan, 0.81118881, 0.81118881,\n",
       "               nan,        nan, 0.81118881,        nan, 0.8041958 ,\n",
       "        0.81818182, 0.81818182, 0.81118881, 0.81818182, 0.81818182,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.79020979,        nan, 0.79020979,\n",
       "        0.77622378, 0.77622378, 0.77622378, 0.77622378, 0.77622378,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78321678, 0.78321678,        nan, 0.78321678, 0.78321678,\n",
       "               nan,        nan, 0.78321678,        nan, 0.79020979,\n",
       "        0.78321678, 0.78321678, 0.77622378, 0.78321678, 0.78321678,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.79020979,        nan, 0.79020979,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81118881, 0.81118881,        nan, 0.81118881, 0.81118881,\n",
       "               nan,        nan, 0.8041958 ,        nan, 0.81118881,\n",
       "        0.81118881, 0.81118881, 0.81118881, 0.81118881, 0.81118881,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.76223776,        nan, 0.76223776,\n",
       "        0.76223776, 0.76223776, 0.76223776, 0.76223776, 0.76223776,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78321678, 0.78321678,        nan, 0.78321678, 0.78321678,\n",
       "               nan,        nan, 0.78321678,        nan, 0.78321678,\n",
       "        0.78321678, 0.78321678, 0.78321678, 0.78321678, 0.78321678,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.76223776,        nan, 0.76223776,\n",
       "        0.76223776, 0.76223776, 0.76223776, 0.76223776, 0.76223776,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81118881, 0.81118881,        nan, 0.81118881, 0.81118881,\n",
       "               nan,        nan, 0.81118881,        nan, 0.81118881,\n",
       "        0.81118881, 0.81118881, 0.81118881, 0.81118881, 0.81118881,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.76923077,        nan, 0.76923077,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78321678, 0.78321678,        nan, 0.78321678, 0.78321678,\n",
       "               nan,        nan, 0.78321678,        nan, 0.78321678,\n",
       "        0.78321678, 0.78321678, 0.78321678, 0.78321678, 0.78321678,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.76923077,        nan, 0.76923077,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81118881, 0.81118881,        nan, 0.81118881, 0.81118881,\n",
       "               nan,        nan, 0.81118881,        nan, 0.81118881,\n",
       "        0.81118881, 0.81118881, 0.81118881, 0.81118881, 0.81118881,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.76923077,        nan, 0.76923077,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78321678, 0.78321678,        nan, 0.78321678, 0.78321678,\n",
       "               nan,        nan, 0.78321678,        nan, 0.78321678,\n",
       "        0.78321678, 0.78321678, 0.78321678, 0.78321678, 0.78321678,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76923077, 0.76923077,        nan, 0.76923077, 0.76923077,\n",
       "               nan,        nan, 0.76923077,        nan, 0.76923077,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.76923077, 0.76923077,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'split1_test_score': array([0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.73239437,        nan, 0.62676056,\n",
       "        0.8028169 , 0.8028169 , 0.76056338, 0.8028169 , 0.8028169 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73239437,        nan, 0.73239437,\n",
       "        0.71830986, 0.71830986, 0.71830986, 0.71830986, 0.71830986,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.73239437,        nan, 0.73239437,\n",
       "        0.73239437, 0.73239437, 0.71830986, 0.73239437, 0.73239437,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73239437,        nan, 0.73239437,\n",
       "        0.72535211, 0.72535211, 0.72535211, 0.72535211, 0.72535211,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78169014,        nan, 0.77464789,\n",
       "        0.76760563, 0.76760563, 0.76056338, 0.76760563, 0.76760563,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73943662,        nan, 0.73943662,\n",
       "        0.73239437, 0.73239437, 0.73239437, 0.73239437, 0.73239437,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.73943662,        nan, 0.73943662,\n",
       "        0.75352113, 0.75352113, 0.75352113, 0.75352113, 0.75352113,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73943662,        nan, 0.73943662,\n",
       "        0.73239437, 0.73239437, 0.73239437, 0.73239437, 0.73239437,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73943662,        nan, 0.73943662,\n",
       "        0.73943662, 0.73943662, 0.73943662, 0.73943662, 0.73943662,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.74647887,        nan, 0.73943662,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73239437,        nan, 0.73239437,\n",
       "        0.73239437, 0.73239437, 0.73239437, 0.73239437, 0.73239437,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78169014, 0.78169014, 0.78873239, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73943662,        nan, 0.73943662,\n",
       "        0.73239437, 0.73239437, 0.73239437, 0.73239437, 0.73239437,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.74647887,        nan, 0.74647887,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73239437,        nan, 0.73239437,\n",
       "        0.73239437, 0.73239437, 0.73239437, 0.73239437, 0.73239437,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73239437,        nan, 0.73239437,\n",
       "        0.73239437, 0.73239437, 0.73239437, 0.73239437, 0.73239437,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.74647887,        nan, 0.74647887,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.73239437, 0.73239437,        nan, 0.73239437, 0.73239437,\n",
       "               nan,        nan, 0.73239437,        nan, 0.73239437,\n",
       "        0.73239437, 0.73239437, 0.73239437, 0.73239437, 0.73239437,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'split2_test_score': array([0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.79577465,        nan, 0.62676056,\n",
       "        0.77464789, 0.77464789, 0.8028169 , 0.77464789, 0.77464789,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.77464789, 0.77464789, 0.77464789, 0.77464789, 0.77464789,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.77464789, 0.77464789, 0.77464789, 0.77464789, 0.77464789,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.8028169 ,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78169014,        nan, 0.78169014,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.79577465,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'split3_test_score': array([0.77464789, 0.77464789,        nan, 0.77464789, 0.77464789,\n",
       "               nan,        nan, 0.75352113,        nan, 0.62676056,\n",
       "        0.74647887, 0.74647887, 0.77464789, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77464789, 0.77464789,        nan, 0.77464789, 0.77464789,\n",
       "               nan,        nan, 0.76760563,        nan, 0.78873239,\n",
       "        0.76760563, 0.76760563, 0.76760563, 0.76760563, 0.76760563,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.73239437,        nan, 0.73239437,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.73943662,        nan, 0.74647887,\n",
       "        0.75352113, 0.75352113, 0.75352113, 0.75352113, 0.75352113,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.73943662,        nan, 0.73943662,\n",
       "        0.73943662, 0.73943662, 0.73943662, 0.73943662, 0.73943662,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77464789, 0.77464789,        nan, 0.77464789, 0.77464789,\n",
       "               nan,        nan, 0.76760563,        nan, 0.76760563,\n",
       "        0.76760563, 0.76760563, 0.76760563, 0.76760563, 0.76760563,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.74647887,        nan, 0.74647887,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.75352113, 0.75352113, 0.75352113, 0.75352113, 0.75352113,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.75352113, 0.75352113, 0.75352113, 0.75352113, 0.75352113,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77464789, 0.77464789,        nan, 0.77464789, 0.77464789,\n",
       "               nan,        nan, 0.76760563,        nan, 0.76760563,\n",
       "        0.77464789, 0.77464789, 0.77464789, 0.77464789, 0.77464789,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.74647887,        nan, 0.74647887,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.75352113, 0.75352113, 0.75352113, 0.75352113, 0.75352113,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.75352113, 0.75352113, 0.75352113, 0.75352113, 0.75352113,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77464789, 0.77464789,        nan, 0.77464789, 0.77464789,\n",
       "               nan,        nan, 0.77464789,        nan, 0.77464789,\n",
       "        0.77464789, 0.77464789, 0.77464789, 0.77464789, 0.77464789,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.74647887, 0.74647887,        nan, 0.74647887, 0.74647887,\n",
       "               nan,        nan, 0.74647887,        nan, 0.74647887,\n",
       "        0.74647887, 0.74647887, 0.74647887, 0.74647887, 0.74647887,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.75352113, 0.75352113, 0.75352113, 0.75352113, 0.75352113,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.75352113, 0.75352113,        nan, 0.75352113, 0.75352113,\n",
       "               nan,        nan, 0.75352113,        nan, 0.75352113,\n",
       "        0.75352113, 0.75352113, 0.75352113, 0.75352113, 0.75352113,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'split4_test_score': array([0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.61971831,\n",
       "        0.78169014, 0.78169014, 0.8028169 , 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.8028169 ,        nan, 0.8028169 ,\n",
       "        0.8028169 , 0.8028169 , 0.79577465, 0.8028169 , 0.8028169 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78169014,        nan, 0.78169014,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78169014,        nan, 0.78169014,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.78169014,        nan, 0.78169014,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.8028169 , 0.8028169 , 0.8028169 , 0.8028169 , 0.8028169 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.8028169 ,        nan, 0.8028169 ,\n",
       "        0.8028169 , 0.8028169 , 0.8028169 , 0.8028169 , 0.8028169 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.8028169 ,        nan, 0.8028169 ,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78873239, 0.78873239,        nan, 0.78873239, 0.78873239,\n",
       "               nan,        nan, 0.78873239,        nan, 0.78873239,\n",
       "        0.78873239, 0.78873239, 0.78873239, 0.78873239, 0.78873239,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79577465, 0.79577465,        nan, 0.79577465, 0.79577465,\n",
       "               nan,        nan, 0.79577465,        nan, 0.79577465,\n",
       "        0.79577465, 0.79577465, 0.79577465, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'mean_test_score': array([0.79181523, 0.79181523,        nan, 0.79181523, 0.79181523,\n",
       "               nan,        nan, 0.77493352,        nan, 0.62447552,\n",
       "        0.78336452, 0.78336452, 0.78760957, 0.78336452, 0.78336452,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76511376, 0.76511376,        nan, 0.76511376, 0.76511376,\n",
       "               nan,        nan, 0.77493352,        nan, 0.77493352,\n",
       "        0.76087856, 0.76087856, 0.76087856, 0.76087856, 0.76087856,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77354477, 0.77354477,        nan, 0.77354477, 0.77354477,\n",
       "               nan,        nan, 0.77493352,        nan, 0.77493352,\n",
       "        0.76790111, 0.76790111, 0.76368561, 0.76790111, 0.76790111,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76793066, 0.76793066,        nan, 0.76793066, 0.76793066,\n",
       "               nan,        nan, 0.77493352,        nan, 0.77493352,\n",
       "        0.76228701, 0.76228701, 0.76228701, 0.76228701, 0.76228701,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79181523, 0.79181523,        nan, 0.79181523, 0.79181523,\n",
       "               nan,        nan, 0.79322368,        nan, 0.79182508,\n",
       "        0.78898848, 0.78898848, 0.78477297, 0.78898848, 0.78898848,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76511376, 0.76511376,        nan, 0.76511376, 0.76511376,\n",
       "               nan,        nan, 0.76649266,        nan, 0.76649266,\n",
       "        0.76369546, 0.76369546, 0.76369546, 0.76369546, 0.76369546,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77354477, 0.77354477,        nan, 0.77354477, 0.77354477,\n",
       "               nan,        nan, 0.76650251,        nan, 0.76930956,\n",
       "        0.77354477, 0.77354477, 0.77214616, 0.77354477, 0.77354477,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76793066, 0.76793066,        nan, 0.76793066, 0.76793066,\n",
       "               nan,        nan, 0.76649266,        nan, 0.76649266,\n",
       "        0.76370531, 0.76370531, 0.76370531, 0.76370531, 0.76370531,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79181523, 0.79181523,        nan, 0.79181523, 0.79181523,\n",
       "               nan,        nan, 0.78900817,        nan, 0.79181523,\n",
       "        0.79040678, 0.79040678, 0.79040678, 0.79040678, 0.79040678,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76511376, 0.76511376,        nan, 0.76511376, 0.76511376,\n",
       "               nan,        nan, 0.76512361,        nan, 0.76512361,\n",
       "        0.76653206, 0.76653206, 0.76653206, 0.76653206, 0.76653206,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77354477, 0.77354477,        nan, 0.77354477, 0.77354477,\n",
       "               nan,        nan, 0.77354477,        nan, 0.77213631,\n",
       "        0.77354477, 0.77354477, 0.77354477, 0.77354477, 0.77354477,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76793066, 0.76793066,        nan, 0.76793066, 0.76793066,\n",
       "               nan,        nan, 0.76794051,        nan, 0.76794051,\n",
       "        0.76794051, 0.76794051, 0.76794051, 0.76794051, 0.76794051,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79181523, 0.79181523,        nan, 0.79181523, 0.79181523,\n",
       "               nan,        nan, 0.79181523,        nan, 0.79181523,\n",
       "        0.79040678, 0.79040678, 0.79181523, 0.79040678, 0.79040678,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76511376, 0.76511376,        nan, 0.76511376, 0.76511376,\n",
       "               nan,        nan, 0.76652221,        nan, 0.76652221,\n",
       "        0.76511376, 0.76511376, 0.76511376, 0.76511376, 0.76511376,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77354477, 0.77354477,        nan, 0.77354477, 0.77354477,\n",
       "               nan,        nan, 0.77354477,        nan, 0.77354477,\n",
       "        0.77354477, 0.77354477, 0.77354477, 0.77354477, 0.77354477,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76793066, 0.76793066,        nan, 0.76793066, 0.76793066,\n",
       "               nan,        nan, 0.76793066,        nan, 0.76793066,\n",
       "        0.76793066, 0.76793066, 0.76793066, 0.76793066, 0.76793066,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79181523, 0.79181523,        nan, 0.79181523, 0.79181523,\n",
       "               nan,        nan, 0.79181523,        nan, 0.79181523,\n",
       "        0.79181523, 0.79181523, 0.79181523, 0.79181523, 0.79181523,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76511376, 0.76511376,        nan, 0.76511376, 0.76511376,\n",
       "               nan,        nan, 0.76511376,        nan, 0.76511376,\n",
       "        0.76511376, 0.76511376, 0.76511376, 0.76511376, 0.76511376,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77354477, 0.77354477,        nan, 0.77354477, 0.77354477,\n",
       "               nan,        nan, 0.77354477,        nan, 0.77354477,\n",
       "        0.77354477, 0.77354477, 0.77354477, 0.77354477, 0.77354477,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76793066, 0.76793066,        nan, 0.76793066, 0.76793066,\n",
       "               nan,        nan, 0.76793066,        nan, 0.76793066,\n",
       "        0.76793066, 0.76793066, 0.76793066, 0.76793066, 0.76793066,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'std_test_score': array([0.01187214, 0.01187214,        nan, 0.01187214, 0.01187214,\n",
       "               nan,        nan, 0.02695439,        nan, 0.00292221,\n",
       "        0.02276805, 0.02276805, 0.0170532 , 0.02276805, 0.02276805,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02258516, 0.02258516,        nan, 0.02258516, 0.02258516,\n",
       "               nan,        nan, 0.02695439,        nan, 0.02695439,\n",
       "        0.02537854, 0.02537854, 0.02537854, 0.02537854, 0.02537854,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01975798, 0.01975798,        nan, 0.01975798, 0.01975798,\n",
       "               nan,        nan, 0.02695439,        nan, 0.02695439,\n",
       "        0.02383862, 0.02383862, 0.0271588 , 0.02383862, 0.02383862,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02315233, 0.02315233,        nan, 0.02315233, 0.02315233,\n",
       "               nan,        nan, 0.02695439,        nan, 0.02695439,\n",
       "        0.02306715, 0.02306715, 0.02306715, 0.02306715, 0.02306715,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01187214, 0.01187214,        nan, 0.01187214, 0.01187214,\n",
       "               nan,        nan, 0.01610039,        nan, 0.01084473,\n",
       "        0.01978886, 0.01978886, 0.01852205, 0.01978886, 0.01978886,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02258516, 0.02258516,        nan, 0.02258516, 0.02258516,\n",
       "               nan,        nan, 0.02523012,        nan, 0.02523012,\n",
       "        0.02039973, 0.02039973, 0.02039973, 0.02039973, 0.02039973,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01975798, 0.01975798,        nan, 0.01975798, 0.01975798,\n",
       "               nan,        nan, 0.02222307,        nan, 0.02182191,\n",
       "        0.01706432, 0.01706432, 0.01649123, 0.01706432, 0.01706432,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02315233, 0.02315233,        nan, 0.02315233, 0.02315233,\n",
       "               nan,        nan, 0.02230914,        nan, 0.02230914,\n",
       "        0.02429721, 0.02429721, 0.02429721, 0.02429721, 0.02429721,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01187214, 0.01187214,        nan, 0.01187214, 0.01187214,\n",
       "               nan,        nan, 0.01212163,        nan, 0.01415838,\n",
       "        0.01539207, 0.01539207, 0.01539207, 0.01539207, 0.01539207,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02258516, 0.02258516,        nan, 0.02258516, 0.02258516,\n",
       "               nan,        nan, 0.02064231,        nan, 0.02064231,\n",
       "        0.02237283, 0.02237283, 0.02237283, 0.02237283, 0.02237283,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01975798, 0.01975798,        nan, 0.01975798, 0.01975798,\n",
       "               nan,        nan, 0.01975798,        nan, 0.02178427,\n",
       "        0.01975798, 0.01975798, 0.01975798, 0.01975798, 0.01975798,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02315233, 0.02315233,        nan, 0.02315233, 0.02315233,\n",
       "               nan,        nan, 0.02511029,        nan, 0.02511029,\n",
       "        0.02511029, 0.02511029, 0.02511029, 0.02511029, 0.02511029,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01187214, 0.01187214,        nan, 0.01187214, 0.01187214,\n",
       "               nan,        nan, 0.0148424 ,        nan, 0.0148424 ,\n",
       "        0.01255255, 0.01255255, 0.01187214, 0.01255255, 0.01255255,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02258516, 0.02258516,        nan, 0.02258516, 0.02258516,\n",
       "               nan,        nan, 0.02063631,        nan, 0.02063631,\n",
       "        0.02258516, 0.02258516, 0.02258516, 0.02258516, 0.02258516,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01975798, 0.01975798,        nan, 0.01975798, 0.01975798,\n",
       "               nan,        nan, 0.01975798,        nan, 0.01975798,\n",
       "        0.01975798, 0.01975798, 0.01975798, 0.01975798, 0.01975798,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02315233, 0.02315233,        nan, 0.02315233, 0.02315233,\n",
       "               nan,        nan, 0.02315233,        nan, 0.02315233,\n",
       "        0.02315233, 0.02315233, 0.02315233, 0.02315233, 0.02315233,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01187214, 0.01187214,        nan, 0.01187214, 0.01187214,\n",
       "               nan,        nan, 0.01187214,        nan, 0.01187214,\n",
       "        0.01187214, 0.01187214, 0.01187214, 0.01187214, 0.01187214,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02258516, 0.02258516,        nan, 0.02258516, 0.02258516,\n",
       "               nan,        nan, 0.02258516,        nan, 0.02258516,\n",
       "        0.02258516, 0.02258516, 0.02258516, 0.02258516, 0.02258516,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.01975798, 0.01975798,        nan, 0.01975798, 0.01975798,\n",
       "               nan,        nan, 0.01975798,        nan, 0.01975798,\n",
       "        0.01975798, 0.01975798, 0.01975798, 0.01975798, 0.01975798,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.02315233, 0.02315233,        nan, 0.02315233, 0.02315233,\n",
       "               nan,        nan, 0.02315233,        nan, 0.02315233,\n",
       "        0.02315233, 0.02315233, 0.02315233, 0.02315233, 0.02315233,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'rank_test_score': array([  4,   4, 310,   4,   4, 311, 312,  54, 313, 220,  50,  50,  48,\n",
       "         50,  50, 314, 315, 316, 317, 318, 167, 167, 319, 167, 167, 320,\n",
       "        321,  54, 322,  54, 215, 215, 215, 215, 215, 323, 324, 325, 326,\n",
       "        327,  61,  61, 328,  61,  61, 329, 330,  54, 331,  54, 149, 149,\n",
       "        209, 149, 149, 332, 333, 334, 335, 336, 115, 115, 309, 115, 115,\n",
       "        337, 308,  54, 306,  54, 210, 210, 210, 210, 210, 279, 280, 281,\n",
       "        282, 283,   4,   4, 284,   4,   4, 285, 286,   1, 287,   2,  44,\n",
       "         44,  49,  44,  44, 288, 289, 290, 291, 292, 167, 167, 293, 167,\n",
       "        167, 294, 295, 163, 296, 163, 204, 204, 204, 204, 204, 297, 298,\n",
       "        299, 300, 301,  61,  61, 302,  61,  61, 303, 304, 160, 305, 107,\n",
       "         61,  61, 105,  61,  61, 307, 339, 369, 340, 372, 115, 115, 373,\n",
       "        115, 115, 374, 375, 161, 376, 161, 199, 199, 199, 199, 199, 377,\n",
       "        378, 379, 380, 381,   4,   4, 382,   4,   4, 383, 384,  43, 385,\n",
       "          3,  34,  34,  34,  34,  34, 386, 387, 388, 389, 390, 167, 167,\n",
       "        391, 167, 167, 392, 393, 165, 394, 165, 153, 153, 153, 153, 153,\n",
       "        395, 396, 397, 398, 277,  61,  61, 399,  61,  61, 370, 368,  61,\n",
       "        341, 106,  61,  61,  61,  61,  61, 342, 343, 344, 345, 346, 115,\n",
       "        115, 347, 115, 115, 348, 349, 108, 350, 108, 108, 108, 108, 108,\n",
       "        108, 351, 352, 353, 354, 355,   4,   4, 356,   4,   4, 357, 358,\n",
       "          4, 359,   4,  34,  34,   4,  34,  34, 360, 361, 362, 363, 364,\n",
       "        167, 167, 365, 167, 167, 366, 367, 158, 278, 158, 167, 167, 167,\n",
       "        167, 167, 338, 276, 222, 223, 228,  61,  61, 224,  61,  61, 226,\n",
       "        227,  61, 225,  61,  61,  61,  61,  61,  61, 221, 252, 230, 255,\n",
       "        256, 115, 115, 257, 115, 115, 258, 259, 115, 260, 115, 115, 115,\n",
       "        115, 115, 115, 261, 262, 263, 265, 275,   4,   4, 266,   4,   4,\n",
       "        267, 268,   4, 269,   4,   4,   4,   4,   4,   4, 270, 271, 229,\n",
       "        272, 273, 167, 167, 274, 167, 167, 254, 264, 167, 253, 167, 167,\n",
       "        167, 167, 167, 167, 240, 231, 232, 233, 234,  61,  61, 235,  61,\n",
       "         61, 236, 237,  61, 238,  61,  61,  61,  61,  61,  61, 239, 241,\n",
       "        251, 242, 243, 115, 115, 244, 115, 115, 245, 246, 115, 247, 115,\n",
       "        115, 115, 115, 115, 115, 248, 249, 250, 371, 400], dtype=int32),\n",
       " 'split0_train_score': array([0.79753521, 0.79753521,        nan, 0.79753521, 0.79753521,\n",
       "               nan,        nan, 0.7693662 ,        nan, 0.625     ,\n",
       "        0.77288732, 0.77288732, 0.79049296, 0.77288732, 0.77288732,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77816901, 0.77816901,        nan, 0.77816901, 0.77816901,\n",
       "               nan,        nan, 0.7693662 ,        nan, 0.7693662 ,\n",
       "        0.7693662 , 0.7693662 , 0.7693662 , 0.7693662 , 0.7693662 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78697183, 0.78697183,        nan, 0.78697183, 0.78697183,\n",
       "               nan,        nan, 0.7693662 ,        nan, 0.7693662 ,\n",
       "        0.77640845, 0.77640845, 0.77640845, 0.77640845, 0.77640845,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78169014, 0.78169014,        nan, 0.78169014, 0.78169014,\n",
       "               nan,        nan, 0.7693662 ,        nan, 0.7693662 ,\n",
       "        0.76760563, 0.76760563, 0.76760563, 0.7693662 , 0.7693662 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79753521, 0.79753521,        nan, 0.79753521, 0.79753521,\n",
       "               nan,        nan, 0.79753521,        nan, 0.79049296,\n",
       "        0.79577465, 0.79577465, 0.79401408, 0.79577465, 0.79577465,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77816901, 0.77816901,        nan, 0.77816901, 0.77816901,\n",
       "               nan,        nan, 0.76760563,        nan, 0.76760563,\n",
       "        0.77816901, 0.77816901, 0.77816901, 0.77816901, 0.77816901,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78697183, 0.78697183,        nan, 0.78697183, 0.78697183,\n",
       "               nan,        nan, 0.7693662 ,        nan, 0.77640845,\n",
       "        0.78521127, 0.78521127, 0.79049296, 0.78521127, 0.78521127,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78169014, 0.78169014,        nan, 0.78169014, 0.78169014,\n",
       "               nan,        nan, 0.77112676,        nan, 0.77112676,\n",
       "        0.77464789, 0.77464789, 0.77464789, 0.77464789, 0.77464789,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79753521, 0.79753521,        nan, 0.79753521, 0.79753521,\n",
       "               nan,        nan, 0.79753521,        nan, 0.79753521,\n",
       "        0.79929577, 0.79929577, 0.79929577, 0.79929577, 0.79929577,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77816901, 0.77816901,        nan, 0.77816901, 0.77816901,\n",
       "               nan,        nan, 0.77640845,        nan, 0.77640845,\n",
       "        0.77816901, 0.77816901, 0.77816901, 0.77816901, 0.77816901,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78697183, 0.78697183,        nan, 0.78697183, 0.78697183,\n",
       "               nan,        nan, 0.7834507 ,        nan, 0.7834507 ,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78169014, 0.78169014,        nan, 0.78169014, 0.78169014,\n",
       "               nan,        nan, 0.77992958,        nan, 0.77992958,\n",
       "        0.77992958, 0.77992958, 0.77992958, 0.77992958, 0.77992958,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79753521, 0.79753521,        nan, 0.79753521, 0.79753521,\n",
       "               nan,        nan, 0.79753521,        nan, 0.79753521,\n",
       "        0.79753521, 0.79753521, 0.79753521, 0.79753521, 0.79753521,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77816901, 0.77816901,        nan, 0.77816901, 0.77816901,\n",
       "               nan,        nan, 0.77816901,        nan, 0.77816901,\n",
       "        0.77816901, 0.77816901, 0.77816901, 0.77816901, 0.77816901,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78697183, 0.78697183,        nan, 0.78697183, 0.78697183,\n",
       "               nan,        nan, 0.78697183,        nan, 0.78697183,\n",
       "        0.78697183, 0.78697183, 0.78697183, 0.78697183, 0.78697183,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78169014, 0.78169014,        nan, 0.78169014, 0.78169014,\n",
       "               nan,        nan, 0.78169014,        nan, 0.78169014,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79753521, 0.79753521,        nan, 0.79753521, 0.79753521,\n",
       "               nan,        nan, 0.79753521,        nan, 0.79753521,\n",
       "        0.79753521, 0.79753521, 0.79753521, 0.79753521, 0.79753521,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77816901, 0.77816901,        nan, 0.77816901, 0.77816901,\n",
       "               nan,        nan, 0.77816901,        nan, 0.77816901,\n",
       "        0.77816901, 0.77816901, 0.77816901, 0.77816901, 0.77816901,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78697183, 0.78697183,        nan, 0.78697183, 0.78697183,\n",
       "               nan,        nan, 0.78697183,        nan, 0.78697183,\n",
       "        0.78697183, 0.78697183, 0.78697183, 0.78697183, 0.78697183,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78169014, 0.78169014,        nan, 0.78169014, 0.78169014,\n",
       "               nan,        nan, 0.78169014,        nan, 0.78169014,\n",
       "        0.78169014, 0.78169014, 0.78169014, 0.78169014, 0.78169014,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'split1_train_score': array([0.80140598, 0.80140598,        nan, 0.80140598, 0.80140598,\n",
       "               nan,        nan, 0.78558875,        nan, 0.62390158,\n",
       "        0.78383128, 0.78383128, 0.78734622, 0.78383128, 0.78383128,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.78558875,        nan, 0.78558875,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79086116, 0.79086116,        nan, 0.79086116, 0.79086116,\n",
       "               nan,        nan, 0.78558875,        nan, 0.78558875,\n",
       "        0.78383128, 0.78383128, 0.78207381, 0.78383128, 0.78383128,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.78558875,        nan, 0.78558875,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80140598, 0.80140598,        nan, 0.80140598, 0.80140598,\n",
       "               nan,        nan, 0.78910369,        nan, 0.79613357,\n",
       "        0.80140598, 0.80140598, 0.79789104, 0.80140598, 0.80140598,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77328647,        nan, 0.77328647,\n",
       "        0.77855888, 0.77855888, 0.77855888, 0.77855888, 0.77855888,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79086116, 0.79086116,        nan, 0.79086116, 0.79086116,\n",
       "               nan,        nan, 0.77680141,        nan, 0.78734622,\n",
       "        0.78558875, 0.78558875, 0.78734622, 0.78558875, 0.78558875,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77680141, 0.77680141, 0.77680141, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80140598, 0.80140598,        nan, 0.80140598, 0.80140598,\n",
       "               nan,        nan, 0.79613357,        nan, 0.79789104,\n",
       "        0.79789104, 0.79789104, 0.80140598, 0.79789104, 0.79789104,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.78207381,        nan, 0.78207381,\n",
       "        0.77855888, 0.77855888, 0.77855888, 0.77855888, 0.77855888,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79086116, 0.79086116,        nan, 0.79086116, 0.79086116,\n",
       "               nan,        nan, 0.79086116,        nan, 0.79086116,\n",
       "        0.79086116, 0.79086116, 0.78910369, 0.79086116, 0.79086116,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.77855888,        nan, 0.77855888,\n",
       "        0.77855888, 0.77855888, 0.77855888, 0.77855888, 0.77855888,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80140598, 0.80140598,        nan, 0.80140598, 0.80140598,\n",
       "               nan,        nan, 0.80140598,        nan, 0.80140598,\n",
       "        0.80140598, 0.80140598, 0.80140598, 0.80140598, 0.80140598,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77680141, 0.77680141, 0.77680141, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79086116, 0.79086116,        nan, 0.79086116, 0.79086116,\n",
       "               nan,        nan, 0.79086116,        nan, 0.79086116,\n",
       "        0.79086116, 0.79086116, 0.79086116, 0.79086116, 0.79086116,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.77855888,        nan, 0.77855888,\n",
       "        0.77855888, 0.77855888, 0.77855888, 0.77855888, 0.77855888,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80140598, 0.80140598,        nan, 0.80140598, 0.80140598,\n",
       "               nan,        nan, 0.80140598,        nan, 0.80140598,\n",
       "        0.80140598, 0.80140598, 0.80140598, 0.80140598, 0.80140598,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77680141, 0.77680141, 0.77680141, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79086116, 0.79086116,        nan, 0.79086116, 0.79086116,\n",
       "               nan,        nan, 0.79086116,        nan, 0.79086116,\n",
       "        0.79086116, 0.79086116, 0.79086116, 0.79086116, 0.79086116,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.77855888,        nan, 0.77855888,\n",
       "        0.78031634, 0.78031634, 0.78031634, 0.78031634, 0.78031634,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'split2_train_score': array([0.80316344, 0.80316344,        nan, 0.80316344, 0.80316344,\n",
       "               nan,        nan, 0.76977153,        nan, 0.62390158,\n",
       "        0.78383128, 0.78383128, 0.79964851, 0.78383128, 0.78383128,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76977153, 0.76977153,        nan, 0.76977153, 0.76977153,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.75219684, 0.75219684, 0.75219684, 0.75219684, 0.75219684,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78558875, 0.78558875,        nan, 0.78558875, 0.78558875,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.77328647, 0.77328647, 0.76625659, 0.77328647, 0.77328647,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76801406, 0.76801406,        nan, 0.76801406, 0.76801406,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.75571178, 0.75571178, 0.75571178, 0.75571178, 0.75571178,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80316344, 0.80316344,        nan, 0.80316344, 0.80316344,\n",
       "               nan,        nan, 0.79086116,        nan, 0.7943761 ,\n",
       "        0.80316344, 0.80316344, 0.80316344, 0.80316344, 0.80316344,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76977153, 0.76977153,        nan, 0.76977153, 0.76977153,\n",
       "               nan,        nan, 0.77328647,        nan, 0.77328647,\n",
       "        0.76625659, 0.76625659, 0.76625659, 0.76625659, 0.76625659,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78558875, 0.78558875,        nan, 0.78558875, 0.78558875,\n",
       "               nan,        nan, 0.77328647,        nan, 0.77855888,\n",
       "        0.78031634, 0.78031634, 0.78207381, 0.78031634, 0.78031634,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76801406, 0.76801406,        nan, 0.76801406, 0.76801406,\n",
       "               nan,        nan, 0.76801406,        nan, 0.76801406,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80316344, 0.80316344,        nan, 0.80316344, 0.80316344,\n",
       "               nan,        nan, 0.80316344,        nan, 0.80140598,\n",
       "        0.80316344, 0.80316344, 0.80316344, 0.80316344, 0.80316344,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76977153, 0.76977153,        nan, 0.76977153, 0.76977153,\n",
       "               nan,        nan, 0.76801406,        nan, 0.76801406,\n",
       "        0.76801406, 0.76801406, 0.76801406, 0.76801406, 0.76801406,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78558875, 0.78558875,        nan, 0.78558875, 0.78558875,\n",
       "               nan,        nan, 0.78207381,        nan, 0.78383128,\n",
       "        0.78383128, 0.78383128, 0.78383128, 0.78383128, 0.78383128,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76801406, 0.76801406,        nan, 0.76801406, 0.76801406,\n",
       "               nan,        nan, 0.76801406,        nan, 0.76801406,\n",
       "        0.76625659, 0.76625659, 0.76625659, 0.76625659, 0.76625659,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80316344, 0.80316344,        nan, 0.80316344, 0.80316344,\n",
       "               nan,        nan, 0.80316344,        nan, 0.80316344,\n",
       "        0.80316344, 0.80316344, 0.80316344, 0.80316344, 0.80316344,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76977153, 0.76977153,        nan, 0.76977153, 0.76977153,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.76977153, 0.76977153, 0.76977153, 0.76977153, 0.76977153,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78558875, 0.78558875,        nan, 0.78558875, 0.78558875,\n",
       "               nan,        nan, 0.78558875,        nan, 0.78558875,\n",
       "        0.78558875, 0.78558875, 0.78558875, 0.78558875, 0.78558875,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76801406, 0.76801406,        nan, 0.76801406, 0.76801406,\n",
       "               nan,        nan, 0.76625659,        nan, 0.76625659,\n",
       "        0.76801406, 0.76801406, 0.76801406, 0.76801406, 0.76801406,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80316344, 0.80316344,        nan, 0.80316344, 0.80316344,\n",
       "               nan,        nan, 0.80316344,        nan, 0.80316344,\n",
       "        0.80316344, 0.80316344, 0.80316344, 0.80316344, 0.80316344,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76977153, 0.76977153,        nan, 0.76977153, 0.76977153,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.76977153, 0.76977153, 0.76977153, 0.76977153, 0.76977153,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78558875, 0.78558875,        nan, 0.78558875, 0.78558875,\n",
       "               nan,        nan, 0.78558875,        nan, 0.78558875,\n",
       "        0.78558875, 0.78558875, 0.78558875, 0.78558875, 0.78558875,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.76801406, 0.76801406,        nan, 0.76801406, 0.76801406,\n",
       "               nan,        nan, 0.76801406,        nan, 0.76801406,\n",
       "        0.76801406, 0.76801406, 0.76801406, 0.76801406, 0.76801406,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'split3_train_score': array([0.81370826, 0.81370826,        nan, 0.81370826, 0.81370826,\n",
       "               nan,        nan, 0.78031634,        nan, 0.62390158,\n",
       "        0.79261863, 0.79261863, 0.79086116, 0.79261863, 0.79261863,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77328647, 0.77328647,        nan, 0.77328647, 0.77328647,\n",
       "               nan,        nan, 0.78031634,        nan, 0.78031634,\n",
       "        0.76625659, 0.76625659, 0.76625659, 0.76625659, 0.76625659,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.78031634,        nan, 0.78031634,\n",
       "        0.76977153, 0.76977153, 0.76449912, 0.76977153, 0.76977153,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.78031634,        nan, 0.78031634,\n",
       "        0.76625659, 0.76625659, 0.76625659, 0.76625659, 0.76625659,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81370826, 0.81370826,        nan, 0.81370826, 0.81370826,\n",
       "               nan,        nan, 0.7943761 ,        nan, 0.79613357,\n",
       "        0.81019332, 0.81019332, 0.80492091, 0.81019332, 0.81019332,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77328647, 0.77328647,        nan, 0.77328647, 0.77328647,\n",
       "               nan,        nan, 0.76801406,        nan, 0.76801406,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77680141,        nan, 0.76977153,\n",
       "        0.77855888, 0.77855888, 0.77328647, 0.77855888, 0.77855888,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.77328647,        nan, 0.77328647,\n",
       "        0.77504394, 0.77504394, 0.77504394, 0.77504394, 0.77504394,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81370826, 0.81370826,        nan, 0.81370826, 0.81370826,\n",
       "               nan,        nan, 0.81370826,        nan, 0.81546573,\n",
       "        0.81370826, 0.81370826, 0.81370826, 0.81370826, 0.81370826,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77328647, 0.77328647,        nan, 0.77328647, 0.77328647,\n",
       "               nan,        nan, 0.77328647,        nan, 0.77328647,\n",
       "        0.77328647, 0.77328647, 0.77328647, 0.77328647, 0.77328647,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77680141, 0.77680141, 0.77680141, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77855888, 0.77855888, 0.77855888, 0.77855888, 0.77855888,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81370826, 0.81370826,        nan, 0.81370826, 0.81370826,\n",
       "               nan,        nan, 0.81370826,        nan, 0.81370826,\n",
       "        0.81370826, 0.81370826, 0.81370826, 0.81370826, 0.81370826,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77328647, 0.77328647,        nan, 0.77328647, 0.77328647,\n",
       "               nan,        nan, 0.77328647,        nan, 0.77328647,\n",
       "        0.77328647, 0.77328647, 0.77328647, 0.77328647, 0.77328647,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77680141, 0.77680141, 0.77680141, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.78031634,        nan, 0.78031634,\n",
       "        0.78031634, 0.78031634, 0.78031634, 0.78031634, 0.78031634,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81370826, 0.81370826,        nan, 0.81370826, 0.81370826,\n",
       "               nan,        nan, 0.81370826,        nan, 0.81370826,\n",
       "        0.81370826, 0.81370826, 0.81370826, 0.81370826, 0.81370826,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77328647, 0.77328647,        nan, 0.77328647, 0.77328647,\n",
       "               nan,        nan, 0.77328647,        nan, 0.77328647,\n",
       "        0.77328647, 0.77328647, 0.77328647, 0.77328647, 0.77328647,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77680141, 0.77680141, 0.77680141, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78031634, 0.78031634,        nan, 0.78031634, 0.78031634,\n",
       "               nan,        nan, 0.78031634,        nan, 0.78031634,\n",
       "        0.78031634, 0.78031634, 0.78031634, 0.78031634, 0.78031634,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'split4_train_score': array([0.81195079, 0.81195079,        nan, 0.81195079, 0.81195079,\n",
       "               nan,        nan, 0.76977153,        nan, 0.62565905,\n",
       "        0.79964851, 0.79964851, 0.78734622, 0.79964851, 0.79964851,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.75746924, 0.75746924, 0.75746924, 0.75746924, 0.75746924,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.76274165, 0.76274165, 0.75922671, 0.76274165, 0.76274165,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.75922671, 0.75922671, 0.75922671, 0.75922671, 0.75922671,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81195079, 0.81195079,        nan, 0.81195079, 0.81195079,\n",
       "               nan,        nan, 0.78383128,        nan, 0.78910369,\n",
       "        0.81370826, 0.81370826, 0.80492091, 0.81370826, 0.81370826,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.76449912,        nan, 0.76449912,\n",
       "        0.76801406, 0.76801406, 0.76801406, 0.76801406, 0.76801406,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.76449912,        nan, 0.76977153,\n",
       "        0.77680141, 0.77680141, 0.77504394, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.76449912,        nan, 0.76449912,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81195079, 0.81195079,        nan, 0.81195079, 0.81195079,\n",
       "               nan,        nan, 0.81019332,        nan, 0.81019332,\n",
       "        0.81195079, 0.81195079, 0.81195079, 0.81195079, 0.81195079,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.771529  ,        nan, 0.771529  ,\n",
       "        0.77328647, 0.77328647, 0.77328647, 0.77328647, 0.77328647,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.77504394,        nan, 0.77328647,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77855888,        nan, 0.77855888,\n",
       "        0.77855888, 0.77855888, 0.77855888, 0.77855888, 0.77855888,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81195079, 0.81195079,        nan, 0.81195079, 0.81195079,\n",
       "               nan,        nan, 0.81195079,        nan, 0.81195079,\n",
       "        0.81195079, 0.81195079, 0.81195079, 0.81195079, 0.81195079,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.771529  ,        nan, 0.771529  ,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.76977153,        nan, 0.76977153,\n",
       "        0.76977153, 0.76977153, 0.76977153, 0.76977153, 0.76977153,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77680141, 0.77680141, 0.77680141, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.81195079, 0.81195079,        nan, 0.81195079, 0.81195079,\n",
       "               nan,        nan, 0.81195079,        nan, 0.81195079,\n",
       "        0.81195079, 0.81195079, 0.81195079, 0.81195079, 0.81195079,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.771529  ,        nan, 0.771529  ,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.771529  , 0.771529  ,        nan, 0.771529  , 0.771529  ,\n",
       "               nan,        nan, 0.771529  ,        nan, 0.771529  ,\n",
       "        0.771529  , 0.771529  , 0.771529  , 0.771529  , 0.771529  ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77680141, 0.77680141,        nan, 0.77680141, 0.77680141,\n",
       "               nan,        nan, 0.77680141,        nan, 0.77680141,\n",
       "        0.77680141, 0.77680141, 0.77680141, 0.77680141, 0.77680141,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'mean_train_score': array([0.80555274, 0.80555274,        nan, 0.80555274, 0.80555274,\n",
       "               nan,        nan, 0.77496287,        nan, 0.62447276,\n",
       "        0.78656341, 0.78656341, 0.79113901, 0.78656341, 0.78656341,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77391148, 0.77391148,        nan, 0.77391148, 0.77391148,\n",
       "               nan,        nan, 0.77496287,        nan, 0.77496287,\n",
       "        0.76336357, 0.76336357, 0.76336357, 0.76336357, 0.76336357,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78235043, 0.78235043,        nan, 0.78235043, 0.78235043,\n",
       "               nan,        nan, 0.77496287,        nan, 0.77496287,\n",
       "        0.77320788, 0.77320788, 0.76969294, 0.77320788, 0.77320788,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77742766, 0.77742766,        nan, 0.77742766, 0.77742766,\n",
       "               nan,        nan, 0.77496287,        nan, 0.77496287,\n",
       "        0.76406594, 0.76406594, 0.76406594, 0.76441805, 0.76441805,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80555274, 0.80555274,        nan, 0.80555274, 0.80555274,\n",
       "               nan,        nan, 0.79114149,        nan, 0.79324798,\n",
       "        0.80484913, 0.80484913, 0.80098208, 0.80484913, 0.80484913,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77391148, 0.77391148,        nan, 0.77391148, 0.77391148,\n",
       "               nan,        nan, 0.76933835,        nan, 0.76933835,\n",
       "        0.77250551, 0.77250551, 0.77250551, 0.77250551, 0.77250551,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78235043, 0.78235043,        nan, 0.78235043, 0.78235043,\n",
       "               nan,        nan, 0.77215092,        nan, 0.77637132,\n",
       "        0.78129533, 0.78129533, 0.78164868, 0.78129533, 0.78129533,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77742766, 0.77742766,        nan, 0.77742766, 0.77742766,\n",
       "               nan,        nan, 0.77074556,        nan, 0.77074556,\n",
       "        0.77391025, 0.77391025, 0.77391025, 0.77391025, 0.77391025,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80555274, 0.80555274,        nan, 0.80555274, 0.80555274,\n",
       "               nan,        nan, 0.80414676,        nan, 0.80449825,\n",
       "        0.80520186, 0.80520186, 0.80590485, 0.80520186, 0.80520186,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77391148, 0.77391148,        nan, 0.77391148, 0.77391148,\n",
       "               nan,        nan, 0.77426236,        nan, 0.77426236,\n",
       "        0.77426298, 0.77426298, 0.77426298, 0.77426298, 0.77426298,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78235043, 0.78235043,        nan, 0.78235043, 0.78235043,\n",
       "               nan,        nan, 0.7816462 ,        nan, 0.7816462 ,\n",
       "        0.7809426 , 0.7809426 , 0.7805911 , 0.7809426 , 0.7809426 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77742766, 0.77742766,        nan, 0.77742766, 0.77742766,\n",
       "               nan,        nan, 0.77637256,        nan, 0.77637256,\n",
       "        0.77637256, 0.77637256, 0.77637256, 0.77637256, 0.77637256,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80555274, 0.80555274,        nan, 0.80555274, 0.80555274,\n",
       "               nan,        nan, 0.80555274,        nan, 0.80555274,\n",
       "        0.80555274, 0.80555274, 0.80555274, 0.80555274, 0.80555274,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77391148, 0.77391148,        nan, 0.77391148, 0.77391148,\n",
       "               nan,        nan, 0.77391148,        nan, 0.77391148,\n",
       "        0.77391148, 0.77391148, 0.77391148, 0.77391148, 0.77391148,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78235043, 0.78235043,        nan, 0.78235043, 0.78235043,\n",
       "               nan,        nan, 0.78199894,        nan, 0.78199894,\n",
       "        0.78199894, 0.78199894, 0.78199894, 0.78199894, 0.78199894,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77742766, 0.77742766,        nan, 0.77742766, 0.77742766,\n",
       "               nan,        nan, 0.77672467,        nan, 0.77672467,\n",
       "        0.77707617, 0.77707617, 0.77707617, 0.77707617, 0.77707617,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.80555274, 0.80555274,        nan, 0.80555274, 0.80555274,\n",
       "               nan,        nan, 0.80555274,        nan, 0.80555274,\n",
       "        0.80555274, 0.80555274, 0.80555274, 0.80555274, 0.80555274,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77391148, 0.77391148,        nan, 0.77391148, 0.77391148,\n",
       "               nan,        nan, 0.77391148,        nan, 0.77391148,\n",
       "        0.77391148, 0.77391148, 0.77391148, 0.77391148, 0.77391148,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.78235043, 0.78235043,        nan, 0.78235043, 0.78235043,\n",
       "               nan,        nan, 0.78235043,        nan, 0.78235043,\n",
       "        0.78235043, 0.78235043, 0.78235043, 0.78235043, 0.78235043,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.77742766, 0.77742766,        nan, 0.77742766, 0.77742766,\n",
       "               nan,        nan, 0.77707617,        nan, 0.77707617,\n",
       "        0.77742766, 0.77742766, 0.77742766, 0.77742766, 0.77742766,\n",
       "               nan,        nan,        nan,        nan,        nan]),\n",
       " 'std_train_score': array([0.00623911, 0.00623911,        nan, 0.00623911, 0.00623911,\n",
       "               nan,        nan, 0.00673486,        nan, 0.00072993,\n",
       "        0.00905373, 0.00905373, 0.00450947, 0.00905373, 0.00905373,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00315228, 0.00315228,        nan, 0.00315228, 0.00315228,\n",
       "               nan,        nan, 0.00673486,        nan, 0.00673486,\n",
       "        0.00735547, 0.00735547, 0.00735547, 0.00735547, 0.00735547,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00710171, 0.00710171,        nan, 0.00710171, 0.00710171,\n",
       "               nan,        nan, 0.00673486,        nan, 0.00673486,\n",
       "        0.00699224, 0.00699224, 0.00832719, 0.00699224, 0.00699224,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00497726, 0.00497726,        nan, 0.00497726, 0.00497726,\n",
       "               nan,        nan, 0.00673486,        nan, 0.00673486,\n",
       "        0.00576604, 0.00576604, 0.00576604, 0.00601962, 0.00601962,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00623911, 0.00623911,        nan, 0.00623911, 0.00623911,\n",
       "               nan,        nan, 0.00466932,        nan, 0.00292203,\n",
       "        0.00638886, 0.00638886, 0.00433347, 0.00638886, 0.00638886,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00315228, 0.00315228,        nan, 0.00315228, 0.00315228,\n",
       "               nan,        nan, 0.00344527,        nan, 0.00344527,\n",
       "        0.00507729, 0.00507729, 0.00507729, 0.00507729, 0.00507729,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00710171, 0.00710171,        nan, 0.00710171, 0.00710171,\n",
       "               nan,        nan, 0.00470846,        nan, 0.0065169 ,\n",
       "        0.00353299, 0.00353299, 0.00669945, 0.00353299, 0.00353299,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00497726, 0.00497726,        nan, 0.00497726, 0.00497726,\n",
       "               nan,        nan, 0.00423684,        nan, 0.00423684,\n",
       "        0.00207504, 0.00207504, 0.00207504, 0.00207504, 0.00207504,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00623911, 0.00623911,        nan, 0.00623911, 0.00623911,\n",
       "               nan,        nan, 0.00688293,        nan, 0.00713331,\n",
       "        0.00648679, 0.00648679, 0.00581174, 0.00648679, 0.00648679,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00315228, 0.00315228,        nan, 0.00315228, 0.00315228,\n",
       "               nan,        nan, 0.00475583,        nan, 0.00475583,\n",
       "        0.0038644 , 0.0038644 , 0.0038644 , 0.0038644 , 0.0038644 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00710171, 0.00710171,        nan, 0.00710171, 0.00710171,\n",
       "               nan,        nan, 0.00557524,        nan, 0.00610415,\n",
       "        0.00652758, 0.00652758, 0.0060109 , 0.00652758, 0.00652758,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00497726, 0.00497726,        nan, 0.00497726, 0.00497726,\n",
       "               nan,        nan, 0.0042956 ,        nan, 0.0042956 ,\n",
       "        0.00508577, 0.00508577, 0.00508577, 0.00508577, 0.00508577,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00623911, 0.00623911,        nan, 0.00623911, 0.00623911,\n",
       "               nan,        nan, 0.00623911,        nan, 0.00623911,\n",
       "        0.00623911, 0.00623911, 0.00623911, 0.00623911, 0.00623911,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00315228, 0.00315228,        nan, 0.00315228, 0.00315228,\n",
       "               nan,        nan, 0.00315228,        nan, 0.00315228,\n",
       "        0.00315228, 0.00315228, 0.00315228, 0.00315228, 0.00315228,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00710171, 0.00710171,        nan, 0.00710171, 0.00710171,\n",
       "               nan,        nan, 0.00765087,        nan, 0.00765087,\n",
       "        0.00765087, 0.00765087, 0.00765087, 0.00765087, 0.00765087,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00497726, 0.00497726,        nan, 0.00497726, 0.00497726,\n",
       "               nan,        nan, 0.00548647,        nan, 0.00548647,\n",
       "        0.00482044, 0.00482044, 0.00482044, 0.00482044, 0.00482044,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00623911, 0.00623911,        nan, 0.00623911, 0.00623911,\n",
       "               nan,        nan, 0.00623911,        nan, 0.00623911,\n",
       "        0.00623911, 0.00623911, 0.00623911, 0.00623911, 0.00623911,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00315228, 0.00315228,        nan, 0.00315228, 0.00315228,\n",
       "               nan,        nan, 0.00315228,        nan, 0.00315228,\n",
       "        0.00315228, 0.00315228, 0.00315228, 0.00315228, 0.00315228,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00710171, 0.00710171,        nan, 0.00710171, 0.00710171,\n",
       "               nan,        nan, 0.00710171,        nan, 0.00710171,\n",
       "        0.00710171, 0.00710171, 0.00710171, 0.00710171, 0.00710171,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00497726, 0.00497726,        nan, 0.00497726, 0.00497726,\n",
       "               nan,        nan, 0.00482044,        nan, 0.00482044,\n",
       "        0.00497726, 0.00497726, 0.00497726, 0.00497726, 0.00497726,\n",
       "               nan,        nan,        nan,        nan,        nan])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0683aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79322367773072"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a5a89ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 0.1,\n",
       " 'logisticregression__class_weight': None,\n",
       " 'logisticregression__fit_intercept': True,\n",
       " 'logisticregression__penalty': 'l1',\n",
       " 'logisticregression__solver': 'liblinear'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89524a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98d22855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler', StandardScaler()),\n",
       "  ('logisticregression',\n",
       "   LogisticRegression(C=0.1, penalty='l1', random_state=1, solver='liblinear'))],\n",
       " 'verbose': False,\n",
       " 'standardscaler': StandardScaler(),\n",
       " 'logisticregression': LogisticRegression(C=0.1, penalty='l1', random_state=1, solver='liblinear'),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'logisticregression__C': 0.1,\n",
       " 'logisticregression__class_weight': None,\n",
       " 'logisticregression__dual': False,\n",
       " 'logisticregression__fit_intercept': True,\n",
       " 'logisticregression__intercept_scaling': 1,\n",
       " 'logisticregression__l1_ratio': None,\n",
       " 'logisticregression__max_iter': 100,\n",
       " 'logisticregression__multi_class': 'auto',\n",
       " 'logisticregression__n_jobs': None,\n",
       " 'logisticregression__penalty': 'l1',\n",
       " 'logisticregression__random_state': 1,\n",
       " 'logisticregression__solver': 'liblinear',\n",
       " 'logisticregression__tol': 0.0001,\n",
       " 'logisticregression__verbose': 0,\n",
       " 'logisticregression__warm_start': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9966da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8539325842696629\n"
     ]
    }
   ],
   "source": [
    "test_score = best_clf.score(X_test, y_test)\n",
    "print('Score: {}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f910f59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_probability_distributions(best_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9ebc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_clf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes=np.linspace(0.1, 1, 10),\n",
    "    cv=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba0e6544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(train_scores, test_scores, train_sizes, expected_score=test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4cc28684",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e49a5ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>HasCabin</th>\n",
       "      <th>HasAge</th>\n",
       "      <th>NewAge</th>\n",
       "      <th>IsMale</th>\n",
       "      <th>EmbarkedQ</th>\n",
       "      <th>EmbarkedS</th>\n",
       "      <th>IsFirstClass</th>\n",
       "      <th>IsSecondClass</th>\n",
       "      <th>IsTicketNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>46.9000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>29.642093</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7958</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.7875</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135.6333</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SibSp  Parch      Fare  HasCabin  HasAge     NewAge  IsMale  EmbarkedQ  \\\n",
       "770      0      0    7.8542         1    True  48.000000       1          0   \n",
       "682      5      2   46.9000         1    True  14.000000       1          0   \n",
       "531      1      1    7.2292         1    True  17.000000       1          0   \n",
       "6        0      0   51.8625         0    True  54.000000       1          0   \n",
       "17       0      0   13.0000         1   False  29.642093       1          0   \n",
       "..     ...    ...       ...       ...     ...        ...     ...        ...   \n",
       "498      0      0    7.7958         1    True  24.000000       1          0   \n",
       "730      0      0   18.7875         1    True  11.000000       1          0   \n",
       "372      0      0  135.6333         1    True  22.000000       1          0   \n",
       "587      0      0    8.0500         1    True  22.000000       1          0   \n",
       "233      0      0   10.5000         1    True  24.000000       1          0   \n",
       "\n",
       "     EmbarkedS  IsFirstClass  IsSecondClass  IsTicketNumber  \n",
       "770          1             0              0               1  \n",
       "682          1             0              0               0  \n",
       "531          0             0              0               1  \n",
       "6            1             1              0               1  \n",
       "17           1             0              1               1  \n",
       "..         ...           ...            ...             ...  \n",
       "498          1             0              0               1  \n",
       "730          0             0              0               1  \n",
       "372          0             1              0               0  \n",
       "587          1             0              0               1  \n",
       "233          1             0              1               0  \n",
       "\n",
       "[111 rows x 12 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[(X_test['IsMale']==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95a1b749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208955223880597"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.score(X_test[(X_test['IsMale']==False)], y_test[(X_test['IsMale']==False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eba1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
